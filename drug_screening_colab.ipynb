{
 "cells": [
  {
   "metadata": {
    "id": "YD4ZPQWoW_Pm"
   },
   "cell_type": "markdown",
   "source": [
    "# ðŸ§¬ðŸ’Š Boltz-2 Pipeline for Drug Screening, Mutant Discovery, and Protein Structure Prediction\n",
    "\n",
    "A comprehensive interface for protein-ligand screening and structure prediction using, with integrated mutation discovery from public\n",
    "databases for rapid assessment of mutation effects on drug binding.\n",
    "\n",
    "## ðŸ”¬ What This Tool Does\n",
    "\n",
    "This notebook provides a complete workflow for computational drug screening:\n",
    "\n",
    "1. ðŸ§¬ Protein Structure Prediction: Uses Boltz-2 state-of-the-art AI to predict 3D protein structures from amino acid sequences\n",
    "2. ðŸ§ª Ligand Binding Analysis: Predicts how small molecules (drugs) bind to proteins and estimates binding strength\n",
    "3. âš—ï¸ Co-factor Integration: Includes essential biological molecules (ATP, NAD, heme) that proteins need to function\n",
    "4. ðŸŽ¯ Targeted Design: Constrains binding to specific protein regions for precision drug design\n",
    "5. ðŸ” Mutation Discovery Mode: Automatically queries public databases (UniProt) to find known mutations for your protein\n",
    "6. ðŸ§¬ Mutation Studies: Generates protein variants to evaluate how mutations affect drug binding\n",
    "7. ðŸ“Š Comprehensive Analysis: Provides binding affinities, confidence scores, and downloadable 3D structures\n",
    "\n",
    "## ðŸ“š Quick Tutorial: How to Use This Tool\n",
    "\n",
    "### Step 1: ðŸ”§ Setup & Environment\n",
    "**Run the Installation & Setup cell first:**\n",
    "- â˜ï¸ **Google Colab mode** (default): Automatically installs dependencies and enables file uploads\n",
    "- ðŸ“ **Local mode**: For running on your own computer with pre-installed packages\n",
    "\n",
    "**Local Mode Setup:**\n",
    "1. Check \\\"Running locally\\\" checkbox in the setup cell\n",
    "2. Place these files in your working directory:\n",
    "   - `protein.fasta` - Protein sequences in FASTA format\n",
    "   - `drug.fasta` - SMILES strings in FASTA format (optional)\n",
    "   - `template.cif` - Template structure file (optional)\n",
    "3. Optionally uncheck \\\"Download prerequisites\\\" if packages are already installed\n",
    "\n",
    "### Step 2: ðŸ§¬ Configure Your Screening\n",
    "Choose your operation mode and configure inputs:\n",
    "\n",
    "ðŸ”¸ **Input Modes:**\n",
    "- **Protein-Drug Screening**: Paste single wild-type protein sequence directly\n",
    "- **Protein-Drug Screening - Upload FASTA File**:\n",
    "  - â˜ï¸ Colab: Upload file with multiple protein sequences\n",
    "  - ðŸ“ Local: Uses `protein.fasta` from working directory\n",
    "- **Mutations Mode**: Start with wild-type sequence and generate specific mutant variants\n",
    "- **Structure-only Mode**: Predict protein structure without ligands (for protein folding studies)\n",
    "\n",
    "ðŸ”¸ **Mutation Discovery Mode:**\n",
    "- **Database Query**: Automatically search UniProt for known mutations of your protein\n",
    "- **Manual Selection**: Choose specific mutations from suggested list or enter your own\n",
    "- **Detailed Analysis**: Get mutation impact predictions and severity assessments\n",
    "\n",
    "ðŸ”¸ **Ligand Input Options:**\n",
    "- **Text Input**: Enter SMILES strings\n",
    "- **Protein-Drug Screening - Upload FASTA File**:\n",
    "  - â˜ï¸ Colab: Upload FASTA format file with named SMILES entries\n",
    "  - ðŸ“ Local: Uses `drug.fasta` from working directory\n",
    "- **Co-factor Integration**: Add biological co-factors (ATP, NAD, heme) for realistic binding\n",
    "\n",
    "### Step 3: âš™ï¸ Advanced Parameters (Optional)\n",
    "Configure additional features:\n",
    "- **Co-factors**: Add essential molecules like ATP, NAD, or heme\n",
    "- **Binding Constraints**: Target specific protein regions\n",
    "- **Post-Translational Modifications**: Add chemical modifications to proteins\n",
    "- **Template Structures**:\n",
    "  - â˜ï¸ Colab: Upload .cif template files\n",
    "  - ðŸ“ Local: Uses `template.cif` from working directory\n",
    "\n",
    "### Step 4: ðŸš€ Configure & Generate\n",
    "Set computational parameters:\n",
    "- **GPU Settings**: Enable/disable GPU acceleration\n",
    "- **Sampling Parameters**: Control prediction quality vs speed\n",
    "- **Error Handling**: Automatic retry settings\n",
    "\n",
    "### Step 5: ðŸŒ  Run Predictions\n",
    "- Execute the Structure Predictions cell\n",
    "- Monitor progress (typically 5-10 minutes per protein-ligand pair using Boltz-2)\n",
    "- System will generate 3D structures and binding predictions\n",
    "\n",
    "### Step 6: ðŸ“Š View Results\n",
    "- Check the Results & Download cell for:\n",
    "  - Binding affinity predictions (how strongly drugs bind)\n",
    "  - 3D structure files (.pdb format)\n",
    "  - Confidence scores and quality metrics\n",
    "  - Downloadable results package\n",
    "\n",
    "## ðŸ  Local Execution vs â˜ï¸ Cloud Execution\n",
    "\n",
    "| Feature | Local Mode ðŸ“ | Colab Mode â˜ï¸ |\n",
    "|---------|---------------|---------------|\n",
    "| **Setup** | Check \\\"Running locally\\\" | Default mode |\n",
    "| **Dependencies** | Pre-installed or skip | Auto-installs |\n",
    "| **File Input** | Predefined files | Interactive uploads |\n",
    "| **Protein Data** | `protein.fasta` | Upload or manual entry |\n",
    "| **Drug Data** | `drug.fasta` | Upload or text input |\n",
    "| **Templates** | `template.cif` | Upload .cif files |\n",
    "| **Resources** | Your hardware | Google's servers |\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title 1ï¸âƒ£ ðŸ”§ Installation & Setup { display-mode: \"form\" }\n",
    "#@markdown Run this cell first to install required dependencies and load all functions:\n",
    "\n",
    "#@markdown **Download prerequisites**: Install required packages. Disable only if running locally with packages pre-installed. It is normal to be disconnected temporarily from the Colab runtime during installation.\n",
    "download_prerequisites = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Running locally**: Skip Colab-specific installations and use local files (protein.fasta, drug.fasta, template.cif)\n",
    "running_locally = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# Minimal approach - work with what's already available in Colab\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "print(\"ðŸ”„ Working with Colab's existing environment...\")\n",
    "\n",
    "# Import core libraries that are pre-installed\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import zipfile\n",
    "import sys\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import requests\n",
    "import urllib.parse\n",
    "import yaml\n",
    "\n",
    "# Try importing pandas/numpy without installing anything new\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"âœ… Pandas and NumPy loaded from Colab\")\n",
    "    PANDAS_AVAILABLE = True\n",
    "    NUMPY_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Pandas/NumPy issue: {e}\")\n",
    "    print(\"ðŸ”„ Continuing without pandas - using basic Python data structures\")\n",
    "    PANDAS_AVAILABLE = False\n",
    "    NUMPY_AVAILABLE = False\n",
    "    # Create minimal substitutes\n",
    "    class MockPandas:\n",
    "        def __init__(self):\n",
    "            self.__version__ = \"unavailable\"\n",
    "    pd = MockPandas()\n",
    "\n",
    "    class MockNumPy:\n",
    "        def __init__(self):\n",
    "            self.__version__ = \"unavailable\"\n",
    "    np = MockNumPy()\n",
    "\n",
    "# Google Colab specific imports - skip if running locally\n",
    "if not running_locally:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        from google.colab import widgets\n",
    "        COLAB_AVAILABLE = True\n",
    "        print(\"âœ… Google Colab environment detected\")\n",
    "    except ImportError:\n",
    "        COLAB_AVAILABLE = False\n",
    "        print(\"âš ï¸ Warning: Google Colab not available. Some features may not work.\")\n",
    "else:\n",
    "    COLAB_AVAILABLE = False\n",
    "    print(\"ðŸ“ Local mode: Using local files instead of Colab uploads\")\n",
    "    # Create mock files module for local execution\n",
    "    class MockFiles:\n",
    "        def upload(self):\n",
    "            print(\"ðŸ“ Local mode: Looking for predefined files in current directory\")\n",
    "            return {}\n",
    "    files = MockFiles()\n",
    "\n",
    "# Try optional packages without forcing installation if download_prerequisites is disabled\n",
    "PLOTLY_AVAILABLE = False\n",
    "YAML_AVAILABLE = False\n",
    "BOLTZ_AVAILABLE = False\n",
    "\n",
    "# Try to import RDKit for enhanced SMILES validation\n",
    "RDKIT_AVAILABLE = False\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, Crippen\n",
    "    RDKIT_AVAILABLE = True\n",
    "    print(\"âœ… RDKit available for enhanced SMILES validation\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ RDKit not available - will use basic SMILES validation\")\n",
    "\n",
    "if download_prerequisites:\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        import plotly.express as px\n",
    "        PLOTLY_AVAILABLE = True\n",
    "        print(\"âœ… Plotly available\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ Plotly not available - will use text-based output\")\n",
    "\n",
    "    try:\n",
    "        import yaml\n",
    "        YAML_AVAILABLE = True\n",
    "        print(\"âœ… YAML available\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ YAML not available - will use custom YAML generation\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"âœ… PyTorch {torch.__version__} available\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ PyTorch not available\")\n",
    "\n",
    "    # Try to install and import Boltz-2 with detailed error logging\n",
    "    print(\"ðŸ” Attempting Boltz-2 installation with detailed logging:\")\n",
    "    try:\n",
    "        print(\"Step 1: Installing Boltz-2...\")\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'boltz'],\n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"âœ… Boltz-2 pip install successful\")\n",
    "\n",
    "        print(\"Step 2: Trying to import boltz...\")\n",
    "        import boltz\n",
    "        BOLTZ_AVAILABLE = True\n",
    "        print(\"âœ… Boltz-2 import successful\")\n",
    "        print(f\"   Boltz version: {getattr(boltz, '__version__', 'unknown')}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as install_error:\n",
    "        print(\"âŒ Boltz-2 pip install failed:\")\n",
    "        print(f\"   Return code: {install_error.returncode}\")\n",
    "        print(f\"   STDOUT: {install_error.stdout}\")\n",
    "        print(f\"   STDERR: {install_error.stderr}\")\n",
    "        BOLTZ_AVAILABLE = False\n",
    "\n",
    "    except ImportError as import_error:\n",
    "        print(\"âŒ Boltz-2 import failed:\")\n",
    "        print(f\"   Error: {import_error}\")\n",
    "        print(\"   Pip install succeeded but import failed\")\n",
    "        BOLTZ_AVAILABLE = False\n",
    "\n",
    "    except Exception as other_error:\n",
    "        print(\"âŒ Boltz-2 unexpected error:\")\n",
    "        print(f\"   Error type: {type(other_error).__name__}\")\n",
    "        print(f\"   Error message: {other_error}\")\n",
    "        BOLTZ_AVAILABLE = False\n",
    "\n",
    "    # If Boltz failed, try alternative installation methods\n",
    "    if not BOLTZ_AVAILABLE:\n",
    "        print(\"ðŸ”„ Trying alternative Boltz-2 installation methods:\")\n",
    "\n",
    "        # Method 1: Try installing without dependencies first\n",
    "        try:\n",
    "            print(\"Method 1: Installing boltz without dependencies...\")\n",
    "            result = subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-deps', 'boltz'],\n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(\"âœ… No-deps install successful, trying import...\")\n",
    "            import boltz\n",
    "            BOLTZ_AVAILABLE = True\n",
    "            print(\"âœ… Boltz-2 working with no-deps install\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ No-deps method failed: {e}\")\n",
    "\n",
    "        # Method 2: Try installing with specific index\n",
    "        if not BOLTZ_AVAILABLE:\n",
    "            try:\n",
    "                print(\"Method 2: Installing from PyPI with specific flags...\")\n",
    "                result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'boltz', '--force-reinstall'],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "                print(\"âœ… Force reinstall successful, trying import...\")\n",
    "                import boltz\n",
    "                BOLTZ_AVAILABLE = True\n",
    "                print(\"âœ… Boltz-2 working with force reinstall\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Force reinstall method failed: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping dependency downloads (running locally or download_prerequisites=False)\")\n",
    "    try:\n",
    "        import boltz\n",
    "        BOLTZ_AVAILABLE = True\n",
    "        print(\"âœ… Boltz-2 already available locally\")\n",
    "    except ImportError:\n",
    "        BOLTZ_AVAILABLE = False\n",
    "        print(\"âš ï¸ Boltz-2 not available locally\")\n",
    "\n",
    "# Print environment info\n",
    "print(f\"ðŸ“Š Environment Info:\")\n",
    "print(f\"  - Python: {sys.version.split()[0]}\")\n",
    "if NUMPY_AVAILABLE:\n",
    "    print(f\"  - NumPy: {np.__version__}\")\n",
    "if PANDAS_AVAILABLE:\n",
    "    print(f\"  - Pandas: {pd.__version__}\")\n",
    "print(f\"  - Requests: {requests.__version__}\")\n",
    "print(f\"  - Local mode: {running_locally}\")\n",
    "print(f\"  - Colab available: {COLAB_AVAILABLE}\")\n",
    "\n",
    "# Constants\n",
    "ESTIMATED_TIME_PER_JOB = 300  # 5 minutes per job in seconds\n",
    "RESULTS_DIR = \"drug_screening_results\"\n",
    "\n",
    "# Additional chemical data\n",
    "COMMON_CCD_CODES = {\n",
    "    'HEM': 'Heme (protoporphyrin IX)',\n",
    "    'ATP': 'Adenosine triphosphate',\n",
    "    'ADP': 'Adenosine diphosphate',\n",
    "    'AMP': 'Adenosine monophosphate',\n",
    "    'NAD': 'Nicotinamide adenine dinucleotide',\n",
    "    'NADP': 'Nicotinamide adenine dinucleotide phosphate',\n",
    "    'FAD': 'Flavin adenine dinucleotide',\n",
    "    'FMN': 'Flavin mononucleotide',\n",
    "    'COA': 'Coenzyme A',\n",
    "    'PLP': 'Pyridoxal 5\\'-phosphate',\n",
    "    'THF': 'Tetrahydrofolate',\n",
    "    'B12': 'Vitamin B12 (cobalamin)'\n",
    "}\n",
    "\n",
    "# Standard amino acids\n",
    "STANDARD_AA = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "# Amino acid properties for mutation analysis\n",
    "AA_PROPERTIES = {\n",
    "    'A': {'name': 'Alanine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'R': {'name': 'Arginine', 'type': 'Polar', 'charge': 'Positive', 'size': 'Large'},\n",
    "    'N': {'name': 'Asparagine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Medium'},\n",
    "    'D': {'name': 'Aspartic acid', 'type': 'Polar', 'charge': 'Negative', 'size': 'Medium'},\n",
    "    'C': {'name': 'Cysteine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'Q': {'name': 'Glutamine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Medium'},\n",
    "    'E': {'name': 'Glutamic acid', 'type': 'Polar', 'charge': 'Negative', 'size': 'Medium'},\n",
    "    'G': {'name': 'Glycine', 'type': 'Special', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'H': {'name': 'Histidine', 'type': 'Polar', 'charge': 'Positive', 'size': 'Medium'},\n",
    "    'I': {'name': 'Isoleucine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Medium'},\n",
    "    'L': {'name': 'Leucine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Medium'},\n",
    "    'K': {'name': 'Lysine', 'type': 'Polar', 'charge': 'Positive', 'size': 'Medium'},\n",
    "    'M': {'name': 'Methionine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Medium'},\n",
    "    'F': {'name': 'Phenylalanine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Large'},\n",
    "    'P': {'name': 'Proline', 'type': 'Special', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'S': {'name': 'Serine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'T': {'name': 'Threonine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Small'},\n",
    "    'W': {'name': 'Tryptophan', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Large'},\n",
    "    'Y': {'name': 'Tyrosine', 'type': 'Polar', 'charge': 'Neutral', 'size': 'Large'},\n",
    "    'V': {'name': 'Valine', 'type': 'Hydrophobic', 'charge': 'Neutral', 'size': 'Small'}\n",
    "}\n",
    "\n",
    "# Helper functions for mutation analysis\n",
    "def classify_mutation_type(wt_aa: str, mut_aa: str) -> str:\n",
    "    \"\"\"Classify mutation based on amino acid properties.\"\"\"\n",
    "    if wt_aa not in AA_PROPERTIES or mut_aa not in AA_PROPERTIES:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    wt_props = AA_PROPERTIES[wt_aa]\n",
    "    mut_props = AA_PROPERTIES[mut_aa]\n",
    "\n",
    "    # Check charge change\n",
    "    if wt_props['charge'] != mut_props['charge']:\n",
    "        return \"Charge change\"\n",
    "\n",
    "    # Check chemical property change\n",
    "    if wt_props['type'] != mut_props['type']:\n",
    "        return \"Chemical change\"\n",
    "\n",
    "    # Check size change\n",
    "    if wt_props['size'] != mut_props['size']:\n",
    "        return \"Size change\"\n",
    "\n",
    "    return \"Conservative\"\n",
    "\n",
    "def assess_mutation_severity(wt_aa: str, mut_aa: str) -> str:\n",
    "    \"\"\"Assess the predicted severity of a mutation.\"\"\"\n",
    "    if wt_aa not in AA_PROPERTIES or mut_aa not in AA_PROPERTIES:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    wt_props = AA_PROPERTIES[wt_aa]\n",
    "    mut_props = AA_PROPERTIES[mut_aa]\n",
    "\n",
    "    # Charge changes are typically severe\n",
    "    if wt_props['charge'] != mut_props['charge']:\n",
    "        return \"High\"\n",
    "\n",
    "    # Proline or glycine involved\n",
    "    if wt_aa == 'P' or mut_aa == 'P' or wt_aa == 'G' or mut_aa == 'G':\n",
    "        return \"High\"\n",
    "\n",
    "    # Large size changes\n",
    "    if (wt_props['size'] == 'Large' and mut_props['size'] == 'Small') or \\\n",
    "       (wt_props['size'] == 'Small' and mut_props['size'] == 'Large'):\n",
    "        return \"Moderate\"\n",
    "\n",
    "    # Chemical property changes\n",
    "    if wt_props['type'] != mut_props['type']:\n",
    "        return \"Moderate\"\n",
    "\n",
    "    return \"Low\"\n",
    "\n",
    "def predict_structural_impact(wt_aa: str, mut_aa: str) -> str:\n",
    "    \"\"\"Predict structural impact of mutation.\"\"\"\n",
    "    if wt_aa not in AA_PROPERTIES or mut_aa not in AA_PROPERTIES:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    wt_props = AA_PROPERTIES[wt_aa]\n",
    "    mut_props = AA_PROPERTIES[mut_aa]\n",
    "\n",
    "    # Proline changes\n",
    "    if wt_aa == 'P' and mut_aa != 'P':\n",
    "        return \"Loss of rigidity\"\n",
    "    elif wt_aa != 'P' and mut_aa == 'P':\n",
    "        return \"Increased rigidity\"\n",
    "\n",
    "    # Glycine changes\n",
    "    if wt_aa == 'G' and mut_aa != 'G':\n",
    "        return \"Loss of flexibility\"\n",
    "    elif wt_aa != 'G' and mut_aa == 'G':\n",
    "        return \"Increased flexibility\"\n",
    "\n",
    "    # Cysteine changes (disulfide bonds)\n",
    "    if wt_aa == 'C' and mut_aa != 'C':\n",
    "        return \"Disulfide loss\"\n",
    "    elif wt_aa != 'C' and mut_aa == 'C':\n",
    "        return \"Potential disulfide\"\n",
    "\n",
    "    # Size changes\n",
    "    if wt_props['size'] != mut_props['size']:\n",
    "        if wt_props['size'] == 'Large' and mut_props['size'] == 'Small':\n",
    "            return \"Cavity formation\"\n",
    "        elif wt_props['size'] == 'Small' and mut_props['size'] == 'Large':\n",
    "            return \"Steric clash\"\n",
    "        else:\n",
    "            return \"Size change\"\n",
    "\n",
    "    return \"Minimal\"\n",
    "\n",
    "def generate_example_mutations_detailed(sequence: str, chain_start: int, max_mutations: int) -> List[Dict]:\n",
    "    \"\"\"Generate example mutations with detailed analysis when database lookup fails.\"\"\"\n",
    "    mutations = []\n",
    "\n",
    "    # Generate mutations for positions throughout the sequence\n",
    "    positions = list(range(10, len(sequence) - 10, max(1, len(sequence) // max_mutations)))[:max_mutations]\n",
    "\n",
    "    common_mutations = ['A', 'V', 'L', 'I', 'F', 'Y', 'W', 'S', 'T', 'N', 'Q', 'R', 'K', 'D', 'E', 'C', 'G', 'P', 'H', 'M']\n",
    "\n",
    "    for i, pos in enumerate(positions):\n",
    "        if pos < len(sequence):\n",
    "            wt_aa = sequence[pos]\n",
    "            mut_aa = common_mutations[i % len(common_mutations)]\n",
    "\n",
    "            if wt_aa != mut_aa:  # Only include if different\n",
    "                input_pos = chain_start + pos\n",
    "                mutation_entry = {\n",
    "                    'mutation_code': f\"{wt_aa}{input_pos}{mut_aa}\",\n",
    "                    'canonical_code': f\"{wt_aa}{input_pos}{mut_aa}\",\n",
    "                    'input_position': input_pos,\n",
    "                    'canonical_position': input_pos,\n",
    "                    'wild_type_aa': wt_aa,\n",
    "                    'mutant_aa': mut_aa,\n",
    "                    'description': f\"Example mutation {wt_aa} to {mut_aa} at position {input_pos}\",\n",
    "                    'evidence_count': 0,\n",
    "                    'change_type': classify_mutation_type(wt_aa, mut_aa),\n",
    "                    'severity': assess_mutation_severity(wt_aa, mut_aa),\n",
    "                    'structural_impact': predict_structural_impact(wt_aa, mut_aa),\n",
    "                    'matches_input': True\n",
    "                }\n",
    "                mutations.append(mutation_entry)\n",
    "\n",
    "    return mutations\n",
    "\n",
    "# Protein parsing functions (from utils.py)\n",
    "def parse_protein_chains(protein_sequence):\n",
    "    \"\"\"\n",
    "    Parse protein sequence into individual chains.\n",
    "    Converts input to uppercase before splitting and storing.\n",
    "    Based on utils.py implementation.\n",
    "\n",
    "    Args:\n",
    "        protein_sequence (str): Protein sequence(s) separated by colons\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries with chain information\n",
    "    \"\"\"\n",
    "    protein_sequence = protein_sequence.upper()\n",
    "    protein_chains = []\n",
    "    if protein_sequence.strip():\n",
    "        # Split by colon to get individual chains\n",
    "        chains = protein_sequence.split(':')\n",
    "\n",
    "        # Validate number of chains (max 23: A-W, X reserved for ligand)\n",
    "        if len(chains) > 23:\n",
    "            raise ValueError(f\"Too many protein chains ({len(chains)}). Maximum allowed is 23 chains (A-W). Chain X is reserved for ligands.\")\n",
    "\n",
    "        for i, chain in enumerate(chains):\n",
    "            chain_id = chr(65 + i)  # A, B, C, ..., W (65-87)\n",
    "            protein_chains.append({\n",
    "                \"protein\": {\n",
    "                    \"id\": chain_id,\n",
    "                    \"sequence\": chain.strip()\n",
    "                }\n",
    "            })\n",
    "    return protein_chains\n",
    "\n",
    "# Mutation Discovery Functions\n",
    "def identify_protein_from_sequence(protein_seq: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Identify protein using multiple search strategies via UniProt API.\n",
    "    Returns basic protein information for mutation lookup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean sequence\n",
    "        clean_seq = re.sub(r'[^A-Z]', '', protein_seq.upper())\n",
    "        if len(clean_seq) < 20:\n",
    "            return {\"error\": \"Sequence too short for reliable identification\"}\n",
    "\n",
    "        seq_length = len(clean_seq)\n",
    "\n",
    "        # Strategy 1: Try exact sequence match first\n",
    "        url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "\n",
    "        # Search for exact sequence first\n",
    "        exact_params = {\n",
    "            'query': f'sequence:\"{clean_seq}\"',\n",
    "            'format': 'json',\n",
    "            'size': 1,\n",
    "            'fields': 'accession,id,protein_name,organism_name,length,sequence'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=exact_params, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "            if results:\n",
    "                result = results[0]\n",
    "                return {\n",
    "                    \"accession\": result.get('primaryAccession', ''),\n",
    "                    \"protein_name\": result.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', 'Unknown'),\n",
    "                    \"organism\": result.get('organism', {}).get('scientificName', 'Unknown'),\n",
    "                    \"similarity\": 1.0,\n",
    "                    \"length\": result.get('sequence', {}).get('length', 0)\n",
    "                }\n",
    "\n",
    "        # Strategy 2: Search by protein family keywords for common proteins\n",
    "        common_protein_patterns = [\n",
    "            ('hemoglobin', ['hemoglobin', 'alpha', 'beta']),\n",
    "            ('myoglobin', ['myoglobin']),\n",
    "            ('insulin', ['insulin']),\n",
    "            ('lysozyme', ['lysozyme']),\n",
    "            ('trypsin', ['trypsin']),\n",
    "            ('chymotrypsin', ['chymotrypsin']),\n",
    "            ('pepsin', ['pepsin']),\n",
    "            ('albumin', ['albumin']),\n",
    "            ('immunoglobulin', ['immunoglobulin', 'antibody']),\n",
    "            ('collagen', ['collagen']),\n",
    "            ('elastin', ['elastin']),\n",
    "            ('keratin', ['keratin']),\n",
    "            ('actin', ['actin']),\n",
    "            ('myosin', ['myosin']),\n",
    "            ('tubulin', ['tubulin']),\n",
    "            ('histone', ['histone'])\n",
    "        ]\n",
    "\n",
    "        # Try to identify by known protein signatures\n",
    "        for protein_type, keywords in common_protein_patterns:\n",
    "            keyword_params = {\n",
    "                'query': f'reviewed:true AND {\" OR \".join(keywords)} AND length:[{seq_length-20} TO {seq_length+20}]',\n",
    "                'format': 'json',\n",
    "                'size': 20,\n",
    "                'fields': 'accession,id,protein_name,organism_name,length,sequence'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=keyword_params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results = data.get('results', [])\n",
    "\n",
    "                # Find best sequence match from keyword results\n",
    "                best_match = None\n",
    "                best_similarity = 0\n",
    "\n",
    "                for result in results:\n",
    "                    seq = result.get('sequence', {}).get('value', '')\n",
    "                    if seq and len(seq) > 0:\n",
    "                        # Use a better similarity calculation\n",
    "                        shorter_len = min(len(clean_seq), len(seq))\n",
    "                        longer_len = max(len(clean_seq), len(seq))\n",
    "\n",
    "                        # Calculate alignment-like similarity\n",
    "                        matches = 0\n",
    "                        for i in range(shorter_len):\n",
    "                            if i < len(clean_seq) and i < len(seq) and clean_seq[i] == seq[i]:\n",
    "                                matches += 1\n",
    "\n",
    "                        # Penalize length differences\n",
    "                        length_penalty = abs(len(clean_seq) - len(seq)) / longer_len\n",
    "                        similarity = (matches / shorter_len) * (1 - length_penalty * 0.5)\n",
    "\n",
    "                        if similarity > best_similarity and similarity > 0.6:\n",
    "                            best_similarity = similarity\n",
    "                            best_match = result\n",
    "\n",
    "                if best_match:\n",
    "                    return {\n",
    "                        \"accession\": best_match.get('primaryAccession', ''),\n",
    "                        \"protein_name\": best_match.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', 'Unknown'),\n",
    "                        \"organism\": best_match.get('organism', {}).get('scientificName', 'Unknown'),\n",
    "                        \"similarity\": best_similarity,\n",
    "                        \"length\": best_match.get('sequence', {}).get('length', 0)\n",
    "                    }\n",
    "\n",
    "        # Strategy 3: Broad search with sequence fragments\n",
    "        # Use first and last 20 amino acids as signature\n",
    "        if len(clean_seq) >= 40:\n",
    "            n_terminus = clean_seq[:20]\n",
    "            c_terminus = clean_seq[-20:]\n",
    "\n",
    "            fragment_params = {\n",
    "                'query': f'reviewed:true AND length:[{seq_length-50} TO {seq_length+50}]',\n",
    "                'format': 'json',\n",
    "                'size': 100,\n",
    "                'fields': 'accession,id,protein_name,organism_name,length,sequence'\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=fragment_params, timeout=20)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                results = data.get('results', [])\n",
    "\n",
    "                best_match = None\n",
    "                best_similarity = 0\n",
    "\n",
    "                for result in results:\n",
    "                    seq = result.get('sequence', {}).get('value', '')\n",
    "                    if seq and len(seq) >= 40:\n",
    "                        # Check N and C terminal similarity\n",
    "                        n_matches = sum(1 for a, b in zip(n_terminus, seq[:20]) if a == b)\n",
    "                        c_matches = sum(1 for a, b in zip(c_terminus, seq[-20:]) if a == b)\n",
    "\n",
    "                        # Calculate overall similarity with terminal emphasis\n",
    "                        terminal_similarity = (n_matches + c_matches) / 40.0\n",
    "\n",
    "                        # Also check internal regions\n",
    "                        internal_matches = 0\n",
    "                        internal_total = 0\n",
    "                        step = max(1, len(clean_seq) // 20)  # Sample every step residues\n",
    "\n",
    "                        for i in range(20, min(len(clean_seq), len(seq)) - 20, step):\n",
    "                            if clean_seq[i] == seq[i]:\n",
    "                                internal_matches += 1\n",
    "                            internal_total += 1\n",
    "\n",
    "                        internal_similarity = internal_matches / max(1, internal_total)\n",
    "\n",
    "                        # Combined similarity score\n",
    "                        combined_similarity = terminal_similarity * 0.6 + internal_similarity * 0.4\n",
    "\n",
    "                        if combined_similarity > best_similarity and combined_similarity > 0.3:\n",
    "                            best_similarity = combined_similarity\n",
    "                            best_match = result\n",
    "\n",
    "                if best_match:\n",
    "                    return {\n",
    "                        \"accession\": best_match.get('primaryAccession', ''),\n",
    "                        \"protein_name\": best_match.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', 'Unknown'),\n",
    "                        \"organism\": best_match.get('organism', {}).get('scientificName', 'Unknown'),\n",
    "                        \"similarity\": best_similarity,\n",
    "                        \"length\": best_match.get('sequence', {}).get('length', 0)\n",
    "                    }\n",
    "\n",
    "        return {\"error\": f\"No protein matches found using multiple search strategies\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error identifying protein: {str(e)}\"}\n",
    "\n",
    "def query_mutations_for_protein_detailed(protein_info: Dict, input_sequence: str, chain_start: int = 1, max_mutations: int = 20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Query known mutations for a protein from public databases with detailed information.\n",
    "    Returns a list of mutation dictionaries with comprehensive details.\n",
    "    \"\"\"\n",
    "    mutations = []\n",
    "\n",
    "    if \"error\" in protein_info:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        accession = protein_info.get(\"accession\", \"\")\n",
    "        if not accession:\n",
    "            return []\n",
    "\n",
    "        # Clean input sequence for position mapping\n",
    "        clean_input_seq = re.sub(r'[^A-Z]', '', input_sequence.upper())\n",
    "\n",
    "        # Query UniProt for variants of this protein\n",
    "        url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "        params = {\n",
    "            'query': f'accession:{accession}',\n",
    "            'format': 'json',\n",
    "            'size': 1,\n",
    "            'fields': 'accession,sequence,ft_variant'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = data.get('results', [])\n",
    "\n",
    "            if results:\n",
    "                result = results[0]\n",
    "                canonical_seq = result.get('sequence', {}).get('value', '')\n",
    "                features = result.get('features', [])\n",
    "\n",
    "                # Create sequence alignment mapping\n",
    "                seq_mapping = create_sequence_mapping(clean_input_seq, canonical_seq, chain_start)\n",
    "\n",
    "                for feature in features:\n",
    "                    if feature.get('type') == 'Natural variant':\n",
    "                        # Extract mutation details\n",
    "                        location = feature.get('location', {})\n",
    "                        canonical_pos = location.get('start', {}).get('value')\n",
    "                        description = feature.get('description', '')\n",
    "                        evidence = feature.get('evidences', [])\n",
    "\n",
    "                        if canonical_pos and description:\n",
    "                            # Map canonical position to input sequence position\n",
    "                            input_pos = map_canonical_to_input_position(canonical_pos, seq_mapping, chain_start)\n",
    "\n",
    "                            # Extract wild-type and mutant amino acids\n",
    "                            wt_aa, mut_aa = extract_mutation_residues(description, canonical_seq, canonical_pos)\n",
    "\n",
    "                            if wt_aa and mut_aa and input_pos:\n",
    "                                # Verify the wild-type residue matches input sequence\n",
    "                                if input_pos <= len(clean_input_seq):\n",
    "                                    actual_wt = clean_input_seq[input_pos - 1] if input_pos > 0 else None\n",
    "\n",
    "                                    mutation_entry = {\n",
    "                                        'mutation_code': f\"{wt_aa}{input_pos}{mut_aa}\",\n",
    "                                        'canonical_code': f\"{wt_aa}{canonical_pos}{mut_aa}\",\n",
    "                                        'input_position': input_pos,\n",
    "                                        'canonical_position': canonical_pos,\n",
    "                                        'wild_type_aa': wt_aa,\n",
    "                                        'mutant_aa': mut_aa,\n",
    "                                        'description': description,\n",
    "                                        'evidence_count': len(evidence),\n",
    "                                        'change_type': classify_mutation_type(wt_aa, mut_aa),\n",
    "                                        'severity': assess_mutation_severity(wt_aa, mut_aa),\n",
    "                                        'structural_impact': predict_structural_impact(wt_aa, mut_aa),\n",
    "                                        'matches_input': actual_wt == wt_aa if actual_wt else False\n",
    "                                    }\n",
    "                                    mutations.append(mutation_entry)\n",
    "\n",
    "        # If no variants found, generate example mutations with detailed info\n",
    "        if not mutations:\n",
    "            mutations = generate_example_mutations_detailed(clean_input_seq, chain_start, max_mutations)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying detailed mutations: {e}\")\n",
    "        # Fallback to example mutations\n",
    "        clean_input_seq = re.sub(r'[^A-Z]', '', input_sequence.upper())\n",
    "        mutations = generate_example_mutations_detailed(clean_input_seq, chain_start, max_mutations)\n",
    "\n",
    "    # Sort by input position and limit results\n",
    "    mutations.sort(key=lambda x: x['input_position'])\n",
    "    return mutations[:max_mutations]\n",
    "\n",
    "def create_sequence_mapping(input_seq: str, canonical_seq: str, chain_start: int) -> Dict[int, int]:\n",
    "    \"\"\"Create mapping between canonical and input sequence positions.\"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    if not canonical_seq:\n",
    "        # Simple 1:1 mapping if no canonical sequence\n",
    "        for i in range(len(input_seq)):\n",
    "            mapping[i + 1] = chain_start + i\n",
    "        return mapping\n",
    "\n",
    "    # Simple alignment - find best offset\n",
    "    best_offset = 0\n",
    "    best_matches = 0\n",
    "\n",
    "    for offset in range(-min(50, len(canonical_seq) // 4), min(50, len(canonical_seq) // 4)):\n",
    "        matches = 0\n",
    "        for i in range(min(len(input_seq), len(canonical_seq) - abs(offset))):\n",
    "            canonical_idx = i + offset if offset >= 0 else i\n",
    "            input_idx = i - offset if offset < 0 else i\n",
    "\n",
    "            if (0 <= canonical_idx < len(canonical_seq) and\n",
    "                0 <= input_idx < len(input_seq) and\n",
    "                canonical_seq[canonical_idx] == input_seq[input_idx]):\n",
    "                matches += 1\n",
    "\n",
    "        if matches > best_matches:\n",
    "            best_matches = matches\n",
    "            best_offset = offset\n",
    "\n",
    "    # Create mapping with best offset\n",
    "    for i in range(len(input_seq)):\n",
    "        canonical_pos = i + best_offset + 1\n",
    "        input_pos = chain_start + i\n",
    "        if 1 <= canonical_pos <= len(canonical_seq):\n",
    "            mapping[canonical_pos] = input_pos\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def map_canonical_to_input_position(canonical_pos: int, mapping: Dict[int, int], chain_start: int) -> Optional[int]:\n",
    "    \"\"\"Map canonical position to input sequence position.\"\"\"\n",
    "    if canonical_pos in mapping:\n",
    "        return mapping[canonical_pos]\n",
    "\n",
    "    # Fallback: assume 1:1 mapping if not found\n",
    "    return canonical_pos - 1 + chain_start if canonical_pos > 0 else None\n",
    "\n",
    "def extract_mutation_residues(description: str, canonical_seq: str, position: int) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Extract wild-type and mutant amino acids from variant description.\"\"\"\n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        r'([A-Z])\\s*->\\s*([A-Z])',  # A -> V\n",
    "        r'([A-Z])(\\d+)([A-Z])',      # A50V\n",
    "        r'p\\.([A-Z])[a-z]*(\\d+)([A-Z])[a-z]*',  # p.Ala50Val\n",
    "        r'([A-Z])[a-z]*\\s*->\\s*([A-Z])[a-z]*'   # Ala -> Val\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, description.upper())\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 2:\n",
    "                return groups[0], groups[-1]\n",
    "\n",
    "    # Fallback: use canonical sequence if position is valid\n",
    "    if canonical_seq and 1 <= position <= len(canonical_seq):\n",
    "        wt_aa = canonical_seq[position - 1]\n",
    "        # Look for any single letter in description that's not the wild-type\n",
    "        for aa in 'ACDEFGHIKLMNPQRSTVWY':\n",
    "            if aa != wt_aa and aa in description.upper():\n",
    "                return wt_aa, aa\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def format_mutations_for_input(mutations: List[Dict]) -> str:\n",
    "    \"\"\"Format mutations as a simple comma-separated list for user input.\"\"\"\n",
    "    if not mutations:\n",
    "        return \"\"\n",
    "\n",
    "    mutation_codes = [m['mutation_code'] for m in mutations if 'mutation_code' in m]\n",
    "    return \", \".join(mutation_codes[:10])  # Limit to first 10 mutations\n",
    "\n",
    "def format_mutations_table(mutations: List[Dict]) -> str:\n",
    "    \"\"\"Format mutations as a readable text table with consistent spacing.\"\"\"\n",
    "    if not mutations:\n",
    "        return \"No mutations found.\"\n",
    "\n",
    "    # Calculate actual maximum widths for each column\n",
    "    mutation_width = max(len(\"Mutation\"), max(len(m['mutation_code']) for m in mutations)) + 1\n",
    "    canonical_width = max(len(\"Canonical\"), max(len(m['canonical_code']) for m in mutations)) + 1\n",
    "    input_pos_width = max(len(\"Input Pos\"), max(len(str(m['input_position'])) for m in mutations)) + 1\n",
    "    canonical_pos_width = max(len(\"Canon Pos\"), max(len(str(m['canonical_position'])) for m in mutations)) + 1\n",
    "    change_width = max(len(\"Change Type\"), max(len(m.get('change_type', '')) for m in mutations)) + 1\n",
    "    severity_width = max(len(\"Severity\"), max(len(m.get('severity', '')) for m in mutations)) + 1\n",
    "    impact_width = max(len(\"Impact\"), max(len(m.get('structural_impact', '')) for m in mutations)) + 1\n",
    "\n",
    "    # Create header\n",
    "    header = (f\"{'Mutation':<{mutation_width}} \"\n",
    "             f\"{'Canonical':<{canonical_width}} \"\n",
    "             f\"{'Input Pos':<{input_pos_width}} \"\n",
    "             f\"{'Canon Pos':<{canonical_pos_width}} \"\n",
    "             f\"{'Change Type':<{change_width}} \"\n",
    "             f\"{'Severity':<{severity_width}} \"\n",
    "             f\"{'Impact':<{impact_width}}\")\n",
    "\n",
    "    # Create separator\n",
    "    separator = \"=\" * len(header)\n",
    "\n",
    "    # Create rows\n",
    "    rows = []\n",
    "    for m in mutations:\n",
    "        row = (f\"{m['mutation_code']:<{mutation_width}} \"\n",
    "               f\"{m['canonical_code']:<{canonical_width}} \"\n",
    "               f\"{m['input_position']:<{input_pos_width}} \"\n",
    "               f\"{m['canonical_position']:<{canonical_pos_width}} \"\n",
    "               f\"{m.get('change_type', ''):<{change_width}} \"\n",
    "               f\"{m.get('severity', ''):<{severity_width}} \"\n",
    "               f\"{m.get('structural_impact', ''):<{impact_width}}\")\n",
    "        rows.append(row)\n",
    "\n",
    "    return \"\\n\".join([header, separator] + rows)\n",
    "\n",
    "def parse_fasta_sequences(fasta_content: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Parse FASTA format sequences into a list of (name, sequence) tuples.\"\"\"\n",
    "    sequences = []\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in fasta_content.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('>'):\n",
    "            # Save previous sequence if exists\n",
    "            if current_id:\n",
    "                sequences.append((current_id, ''.join(current_seq)))\n",
    "\n",
    "            # Start new sequence\n",
    "            current_id = line[1:].split()[0]  # Get first part after >\n",
    "            current_seq = []\n",
    "        elif line and current_id:\n",
    "            # Add to current sequence\n",
    "            current_seq.append(line)\n",
    "\n",
    "    # Save last sequence\n",
    "    if current_id:\n",
    "        sequences.append((current_id, ''.join(current_seq)))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def validate_smiles(smiles: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate SMILES string and return validation results.\"\"\"\n",
    "    if not smiles or not smiles.strip():\n",
    "        return False, \"Empty SMILES string\"\n",
    "\n",
    "    smiles = smiles.strip()\n",
    "\n",
    "    # Use RDKit if available for enhanced validation\n",
    "    if RDKIT_AVAILABLE:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return False, \"Invalid SMILES - RDKit cannot parse\"\n",
    "\n",
    "            # Additional checks\n",
    "            try:\n",
    "                mw = Descriptors.MolWt(mol)\n",
    "                logp = Crippen.MolLogP(mol)\n",
    "                canonical_smiles = Chem.MolToSmiles(mol)\n",
    "                return True, f\"Valid (MW: {mw:.1f}, LogP: {logp:.1f})\"\n",
    "            except:\n",
    "                return True, \"Valid SMILES\"\n",
    "        except Exception as e:\n",
    "            return False, f\"RDKit validation error: {str(e)}\"\n",
    "\n",
    "    # Basic validation without RDKit\n",
    "    try:\n",
    "        # Check for basic SMILES characters\n",
    "        valid_chars = set('CNOPSFClBrI[]()=#+-.0123456789@\\\\/%')\n",
    "        if not all(c in valid_chars for c in smiles):\n",
    "            return False, \"Contains invalid characters for SMILES\"\n",
    "\n",
    "        # Check balanced parentheses and brackets\n",
    "        paren_count = 0\n",
    "        bracket_count = 0\n",
    "        for char in smiles:\n",
    "            if char == '(':\n",
    "                paren_count += 1\n",
    "            elif char == ')':\n",
    "                paren_count -= 1\n",
    "            elif char == '[':\n",
    "                bracket_count += 1\n",
    "            elif char == ']':\n",
    "                bracket_count -= 1\n",
    "\n",
    "            if paren_count < 0 or bracket_count < 0:\n",
    "                return False, \"Unbalanced parentheses or brackets\"\n",
    "\n",
    "        if paren_count != 0 or bracket_count != 0:\n",
    "            return False, \"Unmatched parentheses or brackets\"\n",
    "\n",
    "        return True, \"Valid SMILES (basic validation)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Basic validation error: {str(e)}\"\n",
    "\n",
    "def validate_ccd_code(ccd_code: str) -> bool:\n",
    "    \"\"\"Validate PDB Chemical Component Dictionary (CCD) code.\"\"\"\n",
    "    if not ccd_code or not ccd_code.strip():\n",
    "        return False\n",
    "\n",
    "    ccd_code = ccd_code.strip().upper()\n",
    "\n",
    "    # Basic format validation\n",
    "    if len(ccd_code) < 1 or len(ccd_code) > 5:\n",
    "        return False\n",
    "\n",
    "    # Check if it's a known common code\n",
    "    if ccd_code in COMMON_CCD_CODES:\n",
    "        return True\n",
    "\n",
    "    # Check for valid characters (letters and numbers)\n",
    "    if not ccd_code.replace('_', '').isalnum():\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def parse_mutations(mutation_string: str) -> List[List[Tuple[str, int, str]]]:\n",
    "    \"\"\"Parse mutation string into individual mutations.\"\"\"\n",
    "    if not mutation_string or not mutation_string.strip():\n",
    "        return []\n",
    "\n",
    "    # Split by common delimiters\n",
    "    mutations = []\n",
    "    for delimiter in [',', ';', '\\n', '\\t']:\n",
    "        if delimiter in mutation_string:\n",
    "            mutations = [m.strip() for m in mutation_string.split(delimiter)]\n",
    "            break\n",
    "    else:\n",
    "        # Try space separation if no other delimiter found\n",
    "        mutations = mutation_string.split()\n",
    "\n",
    "    # Filter out empty strings and validate format\n",
    "    mutation_lists = []\n",
    "    for mut in mutations:\n",
    "        mut = mut.strip()\n",
    "        if mut and re.match(r'^[A-Z]\\d+[A-Z](-[A-Z]\\d+[A-Z])*$', mut):\n",
    "            # Parse individual or compound mutations\n",
    "            parts = mut.split('-')\n",
    "            mutation_list = []\n",
    "            for part in parts:\n",
    "                match = re.match(r'^([A-Z])(\\d+)([A-Z])$', part)\n",
    "                if match:\n",
    "                    wt_aa, pos_str, mut_aa = match.groups()\n",
    "                    mutation_list.append(('A', int(pos_str), mut_aa))  # Default to chain A\n",
    "            if mutation_list:\n",
    "                mutation_lists.append(mutation_list)\n",
    "\n",
    "    return mutation_lists\n",
    "\n",
    "def generate_mutant_name(mutations: List[Tuple[str, int, str]]) -> str:\n",
    "    \"\"\"Generate a descriptive name for the mutant protein.\"\"\"\n",
    "    if not mutations:\n",
    "        return \"WT\"\n",
    "\n",
    "    # Convert mutations to standard format\n",
    "    mut_strs = []\n",
    "    for chain_id, position, mut_aa in mutations:\n",
    "        # Get wild-type amino acid from original sequence (simplified)\n",
    "        mut_strs.append(f\"{position}{mut_aa}\")\n",
    "\n",
    "    if len(mut_strs) == 1:\n",
    "        return mut_strs[0]\n",
    "    elif len(mut_strs) <= 3:\n",
    "        return \"-\".join(mut_strs)\n",
    "    else:\n",
    "        return \"-\".join(mut_strs[:2]) + f\"-plus{len(mut_strs)-2}more\"\n",
    "\n",
    "def apply_mutations_to_sequence(wt_sequence: str, mutations: List[Tuple[str, int, str]], chain_starts: Dict[str, int], chains_dict: Dict[str, str] = None) -> str:\n",
    "    \"\"\"Apply mutations to a protein sequence.\"\"\"\n",
    "    if not mutations or not wt_sequence:\n",
    "        return wt_sequence\n",
    "\n",
    "    # Handle multi-chain vs single-chain\n",
    "    if ':' in wt_sequence and chains_dict:\n",
    "        # Multi-chain sequence\n",
    "        chain_sequences = {chain_id: list(seq) for chain_id, seq in chains_dict.items()}\n",
    "\n",
    "        for chain_id, position, mut_aa in mutations:\n",
    "            if chain_id in chain_sequences and chain_id in chain_starts:\n",
    "                chain_start = chain_starts[chain_id]\n",
    "                seq_index = position - chain_start\n",
    "\n",
    "                if 0 <= seq_index < len(chain_sequences[chain_id]):\n",
    "                    chain_sequences[chain_id][seq_index] = mut_aa\n",
    "\n",
    "        # Reconstruct sequence with colons\n",
    "        return ':'.join(''.join(chain_sequences[chain_id]) for chain_id in sorted(chain_sequences.keys()))\n",
    "    else:\n",
    "        # Single chain sequence\n",
    "        seq_list = list(wt_sequence)\n",
    "        chain_start = chain_starts.get('A', 1)\n",
    "\n",
    "        for chain_id, position, mut_aa in mutations:\n",
    "            seq_index = position - chain_start\n",
    "            if 0 <= seq_index < len(seq_list):\n",
    "                seq_list[seq_index] = mut_aa\n",
    "\n",
    "        return ''.join(seq_list)\n",
    "\n",
    "def create_screening_yaml(workspace_name: str, design_name: str, protein_sequence: str, ligand_smiles: str,\n",
    "                         project_name: str, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Create YAML configuration for drug screening using the same logic as utils.py.\n",
    "    Based on create_boltz_yaml from utils.py and create_screening_boltz_yaml from drug_screening.py.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract additional parameters\n",
    "    cofactor_info = kwargs.get('cofactor_info', [])\n",
    "    binding_pocket_constraints = kwargs.get('binding_pocket_constraints')\n",
    "    ptm_modifications = kwargs.get('ptm_modifications')\n",
    "    template_cif_path = kwargs.get('template_cif_path')\n",
    "    structure_only = kwargs.get('structure_only', False)\n",
    "\n",
    "    # Create project directory (similar to drug_screening.py)\n",
    "    project_dir = os.path.join(RESULTS_DIR, project_name)\n",
    "    os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"{workspace_name}_{design_name}.yaml\"\n",
    "    filepath = os.path.join(project_dir, filename)\n",
    "\n",
    "    # Parse protein sequence into chains (from utils.py)\n",
    "    try:\n",
    "        protein_chains = parse_protein_chains(protein_sequence)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Invalid protein sequence format: {str(e)}\")\n",
    "\n",
    "    # Add PTM modifications to protein chains if provided (from utils.py)\n",
    "    if ptm_modifications and ptm_modifications.get('modifications'):\n",
    "        modifications = ptm_modifications.get('modifications', [])\n",
    "        for mod in modifications:\n",
    "            chain_id = mod.get('chain_id', 'A')\n",
    "            position = mod.get('position')\n",
    "            ccd = mod.get('ccd')\n",
    "\n",
    "            if chain_id and position and ccd:\n",
    "                # Find the corresponding protein chain and add modification\n",
    "                for chain in protein_chains:\n",
    "                    if chain.get('protein', {}).get('id') == chain_id:\n",
    "                        if 'modifications' not in chain['protein']:\n",
    "                            chain['protein']['modifications'] = []\n",
    "                        chain['protein']['modifications'].append({\n",
    "                            'position': position,\n",
    "                            'ccd': ccd\n",
    "                        })\n",
    "                        break\n",
    "\n",
    "    # Create YAML content with protein chains (from utils.py)\n",
    "    yaml_content = {\"sequences\": protein_chains}\n",
    "\n",
    "    # Add main ligand if not structure-only mode\n",
    "    if not structure_only and ligand_smiles:\n",
    "        yaml_content[\"sequences\"].append({\n",
    "            \"ligand\": {\n",
    "                \"id\": \"X\",\n",
    "                \"smiles\": ligand_smiles\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Add co-factors if provided (from utils.py)\n",
    "    if cofactor_info and isinstance(cofactor_info, list) and len(cofactor_info) > 0:\n",
    "        for i, cofactor in enumerate(cofactor_info):\n",
    "            if i >= 4:  # Limit to 4 cofactors\n",
    "                break\n",
    "            if cofactor and (cofactor.get('smiles') or cofactor.get('ccd')):\n",
    "                # Use chain IDs T, U, V, W\n",
    "                chain_id = chr(ord('T') + i)\n",
    "                cofactor_entry = {\n",
    "                    \"ligand\": {\n",
    "                        \"id\": chain_id\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Add either SMILES or CCD code\n",
    "                if cofactor.get('smiles'):\n",
    "                    cofactor_entry[\"ligand\"][\"smiles\"] = cofactor['smiles']\n",
    "                elif cofactor.get('ccd'):\n",
    "                    cofactor_entry[\"ligand\"][\"ccd\"] = cofactor['ccd']\n",
    "\n",
    "                # Add co-factor to sequences\n",
    "                yaml_content[\"sequences\"].append(cofactor_entry)\n",
    "    # Backward compatibility: handle single cofactor dict (from utils.py)\n",
    "    elif cofactor_info and isinstance(cofactor_info, dict) and (cofactor_info.get('smiles') or cofactor_info.get('ccd')):\n",
    "        cofactor_entry = {\n",
    "            \"ligand\": {\n",
    "                \"id\": \"T\"  # Use T for backward compatibility\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add either SMILES or CCD code\n",
    "        if cofactor_info.get('smiles'):\n",
    "            cofactor_entry[\"ligand\"][\"smiles\"] = cofactor_info['smiles']\n",
    "        elif cofactor_info.get('ccd'):\n",
    "            cofactor_entry[\"ligand\"][\"ccd\"] = cofactor_info['ccd']\n",
    "\n",
    "        # Add co-factor to sequences\n",
    "        yaml_content[\"sequences\"].append(cofactor_entry)\n",
    "\n",
    "    # Add templates section if template_cif_path is provided (from utils.py)\n",
    "    if template_cif_path:\n",
    "        yaml_content[\"templates\"] = [{\"cif\": os.path.abspath(template_cif_path)}]\n",
    "\n",
    "    # Add constraints if provided and valid (from utils.py)\n",
    "    if binding_pocket_constraints and binding_pocket_constraints.get('contacts'):\n",
    "        contacts = []\n",
    "        for c in binding_pocket_constraints.get('contacts', []):\n",
    "            if len(c) >= 2:\n",
    "                # Try to cast residue index to int if possible\n",
    "                try:\n",
    "                    res_idx = int(c[1])\n",
    "                except (ValueError, TypeError):\n",
    "                    res_idx = c[1]\n",
    "                contacts.append([c[0], res_idx])\n",
    "        pocket_constraint = {\n",
    "            \"pocket\": {\n",
    "                \"binder\": binding_pocket_constraints.get('binder', 'X'),\n",
    "                \"contacts\": contacts,\n",
    "                \"max_distance\": float(binding_pocket_constraints.get('max_distance', 5.0))\n",
    "            }\n",
    "        }\n",
    "        # Create a copy to avoid modifying original (from utils.py)\n",
    "        yaml_content_copy = copy.deepcopy(yaml_content)\n",
    "        yaml_content_copy[\"constraints\"] = [pocket_constraint]\n",
    "        constraints = yaml_content_copy.pop(\"constraints\")\n",
    "\n",
    "        # Write YAML with custom constraints formatting (from utils.py)\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(yaml_content_copy, f, default_flow_style=False)\n",
    "            f.write(\"constraints:\\n\")\n",
    "            for constraint in constraints:\n",
    "                f.write(\"  - pocket:\\n\")\n",
    "                f.write(f\"      binder: {constraint['pocket']['binder']}\\n\")\n",
    "                contacts_str = yaml.dump(constraint['pocket']['contacts'], default_flow_style=True).strip()\n",
    "                f.write(f\"      contacts: {contacts_str}\\n\")\n",
    "                f.write(f\"      max_distance: {constraint['pocket']['max_distance']}\\n\")\n",
    "            # Always append the properties block for affinity prediction (from utils.py)\n",
    "            if not structure_only:\n",
    "                f.write(\"properties:\\n\")\n",
    "                f.write(\"  - affinity:\\n\")\n",
    "                f.write(\"      binder: X\\n\")\n",
    "    else:\n",
    "        # Write standard YAML without constraints (from utils.py)\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "            # Always append the properties block for affinity prediction (from utils.py)\n",
    "            if not structure_only:\n",
    "                f.write(\"properties:\\n\")\n",
    "                f.write(\"  - affinity:\\n\")\n",
    "                f.write(\"      binder: X\\n\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# Local file handling functions\n",
    "def read_local_file(filename: str) -> Optional[str]:\n",
    "    \"\"\"Read a local file if it exists\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error reading {filename}: {e}\")\n",
    "    return None\n",
    "\n",
    "def parse_fasta_from_local_file(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Parse FASTA format from a local file\"\"\"\n",
    "    content = read_local_file(filename)\n",
    "    if not content:\n",
    "        return []\n",
    "\n",
    "    sequences = []\n",
    "    current_name = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('>'):\n",
    "            if current_name and current_seq:\n",
    "                sequences.append((current_name, ''.join(current_seq)))\n",
    "            current_name = line[1:]  # Remove '>'\n",
    "            current_seq = []\n",
    "        elif line:\n",
    "            current_seq.append(line)\n",
    "\n",
    "    # Add last sequence\n",
    "    if current_name and current_seq:\n",
    "        sequences.append((current_name, ''.join(current_seq)))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "print(\"âœ… All functions ready! YAML creation updated to match utils.py reference implementation:\")\n",
    "print(\"ðŸ”§ Features implemented:\")\n",
    "print(\"   - Proper protein chain parsing (A-W, X reserved for ligand)\")\n",
    "print(\"   - PTM modifications support\")\n",
    "print(\"   - Multi-cofactor support (T, U, V, W chain IDs)\")\n",
    "print(\"   - Binding pocket constraints with custom YAML formatting\")\n",
    "print(\"   - Template CIF file support\")\n",
    "print(\"   - Properties block for affinity prediction\")\n",
    "print(\"   - Structure-only mode support\")\n",
    "if running_locally:\n",
    "    print(\"ðŸ“ Local files expected:\")\n",
    "    print(\"   - protein.fasta: Protein sequences in FASTA format\")\n",
    "    print(\"   - drug.fasta: SMILES strings in FASTA format\")\n",
    "    print(\"   - template.cif: Template structure file\")\n",
    "else:\n",
    "    print(\"â˜ï¸ Colab mode: File uploads available\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title 2ï¸âƒ£ ðŸ“± Basic Input Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## ðŸ“¦ Project Settings\n",
    "#@markdown Configure your screening project with a unique name for organizing results and outputs.\n",
    "\n",
    "#@markdown **Project name**: Unique identifier for your screening project (used in output files and results organization)\n",
    "project_name = \"my_drug_screening\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ## ðŸ§¬ Protein Input\n",
    "#@markdown Define how to specify your protein sequences for screening. Choose between different input modes and mutation discovery options:\n",
    "#@markdown\n",
    "#@markdown ðŸ”´ **Protein-Drug Screening - Manual Protein Entry** = paste single wild-type sequence directly (for screening one protein against multiple ligands)\n",
    "#@markdown\n",
    "#@markdown ðŸ”´ **Protein-Drug Screening - Upload FASTA File** = upload file with multiple protein sequences (for screening many proteins against multiple ligands; if running local mode, the script will look for **protein.fasta** in working directory)\n",
    "#@markdown\n",
    "#@markdown ðŸ”µ **Mutants-Drug Screening** = start with wild-type sequence and generate specific mutant variants using database discovery or manual entry\n",
    "input_method = \"Protein-Drug Screening - Manual Protein Entry\"  #@param [\"Protein-Drug Screening - Manual Protein Entry\", \"Protein-Drug Screening - Upload FASTA File\", \"Mutants-Drug Screening\"]\n",
    "\n",
    "#@markdown **Structure-only mode**: Predict protein structure without ligands. Enable for protein folding studies only.\n",
    "structure_only = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ðŸ”´ Protein-Drug Screening - Manual Protein Entry (if selected as input method)\n",
    "#@markdown Paste a single protein sequence directly. Use this for one wild-type protein or when you want to enter the sequence manually.\n",
    "\n",
    "#@markdown Protein sequence: Single letter amino acid codes (A-Z). For multi-chain: use colon-separated format like 'SEQUENCE1:SEQUENCE2:SEQUENCE3'.\n",
    "protein_sequence = \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Protein name**: Descriptive name for your protein (used in output files and results).\n",
    "protein_name = \"Hemoglobin_Alpha\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ðŸ”µ Mutants-Drug Screening (if selected as input method)\n",
    "#@markdown Start with a wild-type sequence and automatically generate specific mutant variants. Use this to study the effects of specific amino acid changes.\n",
    "\n",
    "#@markdown **Wild-type sequence**: Original protein sequence for generating mutants. For multi-chain proteins, use colon (:) to separate chains (e.g., CHAIN1:CHAIN2:CHAIN3).\n",
    "wt_protein_sequence = \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHGKKVADALTNAVAHVDDMPNALSALSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPAEFTPAVHASLDKFLASVSTVLTSKYR\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ðŸ” **Discover mutations from databases (optional)**: If you don't know what mutations to test, enable this to search public databases for known mutations of your protein sequence. After running the cell, copy the suggested mutations to the mutations input field below.\n",
    "query_mutations_from_database = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ðŸ” **Discover mutations from databases - Residue range filter (optional)**: Filter mutations by residue ranges per chain. Format: (start-end),(start-end),... for each chain. Example: (1-50),(100-200) finds mutations in residues 1-50 of chain 1 and 100-200 of chain 2. Leave empty to include all residues.\n",
    "residue_range_filter = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Chain start numbers**: Comma-separated starting residue numbers for each chain (default: 1 for all chains). Format: '1,1,1' for 3 chains all starting at 1, or '1,150,300' for chains starting at different positions.\n",
    "chain_start_numbers = \"1\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Mutations**: Comma-separated list of mutations. Single mutations: A50V. Double mutations: A50V-G60P (format: A50V means Alanine at position 50 changed to Valine). ðŸ’¡ If you don't know what mutations to try, check the box above, run this cell to get suggested mutations, then paste them here.\n",
    "mutations_input = \"A50V, L60P, V70I\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ## ðŸ§ª Ligand SMILES Input\n",
    "#@markdown Specify small molecules for binding analysis (skip if structure-only mode). Choose between:\n",
    "#@markdown\n",
    "#@markdown **Text Input** = enter SMILES strings separated by semicolons (;)\n",
    "#@markdown\n",
    "#@markdown **Upload FASTA File** = upload FASTA format file containing multiple named SMILES entries for easier multi-line input (local mode: looks for **drug.fasta** in working directory).\n",
    "\n",
    "ligand_input_method = \"Text Input\"  #@param [\"Text Input\", \"Upload FASTA File\"]\n",
    "\n",
    "#@markdown **SMILES strings**: Separate multiple SMILES with semicolons (;). Format options: 'SMILES1;SMILES2;SMILES3' or 'Name1,SMILES1;Name2,SMILES2'. Examples: 'CCO;CCN;CC(C)O' or 'Ethanol,CCO;Ethylamine,CCN;Isopropanol,CC(C)O'.\n",
    "ligand_smiles_input = \"CCO;\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown Click RUN â–¶ï¸ to validate and upload your protein/ligand inputs. Then proceed to the next cell for additional parameters.\n",
    "\n",
    "# Additional helper functions for improved mutation analysis\n",
    "def parse_chain_start_numbers(chain_start_input: str, num_chains: int) -> Dict[str, int]:\n",
    "    \"\"\"Parse chain start numbers input into a dictionary mapping chain IDs to start numbers.\"\"\"\n",
    "    chain_starts = {}\n",
    "\n",
    "    if not chain_start_input.strip():\n",
    "        # Default to 1 for all chains\n",
    "        for i in range(num_chains):\n",
    "            if i < 23:  # A-W\n",
    "                chain_id = chr(65 + i)\n",
    "            else:  # Y-Z\n",
    "                chain_id = chr(89 + (i - 23))\n",
    "            chain_starts[chain_id] = 1\n",
    "        return chain_starts\n",
    "\n",
    "    # Parse comma-separated values\n",
    "    start_numbers = [s.strip() for s in chain_start_input.split(',')]\n",
    "\n",
    "    for i in range(num_chains):\n",
    "        if i < 23:  # A-W\n",
    "            chain_id = chr(65 + i)\n",
    "        else:  # Y-Z\n",
    "            chain_id = chr(89 + (i - 23))\n",
    "\n",
    "        # Use provided start number or default to 1\n",
    "        if i < len(start_numbers):\n",
    "            try:\n",
    "                chain_starts[chain_id] = int(start_numbers[i])\n",
    "            except ValueError:\n",
    "                chain_starts[chain_id] = 1\n",
    "        else:\n",
    "            chain_starts[chain_id] = 1\n",
    "\n",
    "    return chain_starts\n",
    "\n",
    "def parse_residue_range_filter(range_filter_input: str, num_chains: int) -> Dict[str, List[Tuple[int, int]]]:\n",
    "    \"\"\"Parse residue range filter input into chain-specific ranges.\"\"\"\n",
    "    if not range_filter_input.strip():\n",
    "        return {}\n",
    "\n",
    "    ranges_per_chain = {}\n",
    "\n",
    "    # Extract ranges in format (start-end),(start-end),...\n",
    "    range_pattern = r'\\((\\d+)-(\\d+)\\)'\n",
    "    matches = re.findall(range_pattern, range_filter_input)\n",
    "\n",
    "    for i, (start_str, end_str) in enumerate(matches):\n",
    "        if i < num_chains:\n",
    "            if i < 23:  # A-W\n",
    "                chain_id = chr(65 + i)\n",
    "            else:  # Y-Z\n",
    "                chain_id = chr(89 + (i - 23))\n",
    "\n",
    "            try:\n",
    "                start_pos = int(start_str)\n",
    "                end_pos = int(end_str)\n",
    "                if start_pos <= end_pos:\n",
    "                    if chain_id not in ranges_per_chain:\n",
    "                        ranges_per_chain[chain_id] = []\n",
    "                    ranges_per_chain[chain_id].append((start_pos, end_pos))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return ranges_per_chain\n",
    "\n",
    "def filter_mutations_by_range(mutations: List[Dict], range_filter: Dict[str, List[Tuple[int, int]]], chain_starts: Dict[str, int]) -> List[Dict]:\n",
    "    \"\"\"Filter mutations based on residue range criteria.\"\"\"\n",
    "    if not range_filter:\n",
    "        return mutations\n",
    "\n",
    "    filtered_mutations = []\n",
    "\n",
    "    for mutation in mutations:\n",
    "        input_pos = mutation.get('input_position', 0)\n",
    "\n",
    "        # Determine which chain this position belongs to\n",
    "        target_chain = None\n",
    "        for chain_id, chain_start in chain_starts.items():\n",
    "            if chain_id in range_filter:\n",
    "                # Check if position falls within any range for this chain\n",
    "                for start_range, end_range in range_filter[chain_id]:\n",
    "                    adjusted_start = start_range + chain_start - 1\n",
    "                    adjusted_end = end_range + chain_start - 1\n",
    "                    if adjusted_start <= input_pos <= adjusted_end:\n",
    "                        target_chain = chain_id\n",
    "                        break\n",
    "                if target_chain:\n",
    "                    break\n",
    "\n",
    "        if target_chain:\n",
    "            filtered_mutations.append(mutation)\n",
    "\n",
    "    return filtered_mutations\n",
    "\n",
    "def format_mutations_table_improved(mutations: List[Dict]) -> str:\n",
    "    \"\"\"Format mutations as a readable text table without redundant position columns.\"\"\"\n",
    "    if not mutations:\n",
    "        return \"No mutations found.\"\n",
    "\n",
    "    # Calculate maximum widths for each column (removed Input Pos and Canon Pos)\n",
    "    mutation_width = max(len(\"Mutation\"), max(len(m['mutation_code']) for m in mutations)) + 1\n",
    "    change_width = max(len(\"Change Type\"), max(len(m.get('change_type', '')) for m in mutations)) + 1\n",
    "    severity_width = max(len(\"Severity\"), max(len(m.get('severity', '')) for m in mutations)) + 1\n",
    "    impact_width = max(len(\"Structural Impact\"), max(len(m.get('structural_impact', '')) for m in mutations)) + 1\n",
    "    evidence_width = max(len(\"Evidence\"), max(len(str(m.get('evidence_count', 0))) for m in mutations)) + 1\n",
    "\n",
    "    # Create header (removed redundant position columns)\n",
    "    header = (f\"{'Mutation':<{mutation_width}} \"\n",
    "             f\"{'Change Type':<{change_width}} \"\n",
    "             f\"{'Severity':<{severity_width}} \"\n",
    "             f\"{'Structural Impact':<{impact_width}} \"\n",
    "             f\"{'Evidence':<{evidence_width}}\")\n",
    "\n",
    "    # Create separator\n",
    "    separator = \"=\" * len(header)\n",
    "\n",
    "    # Create rows\n",
    "    rows = []\n",
    "    for m in mutations:\n",
    "        row = (f\"{m['mutation_code']:<{mutation_width}} \"\n",
    "               f\"{m.get('change_type', ''):<{change_width}} \"\n",
    "               f\"{m.get('severity', ''):<{severity_width}} \"\n",
    "               f\"{m.get('structural_impact', ''):<{impact_width}} \"\n",
    "               f\"{m.get('evidence_count', 0):<{evidence_width}}\")\n",
    "        rows.append(row)\n",
    "\n",
    "    return \"\\n\".join([header, separator] + rows)\n",
    "\n",
    "# Import required validation functions from drug_screening_input.py logic\n",
    "import re\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def validate_protein_sequence(protein_seq: str):\n",
    "    \"\"\"\n",
    "    Validate protein sequence input for multi-chain format using colon separation.\n",
    "    Based on drug_screening_input.py validation logic.\n",
    "    \"\"\"\n",
    "    protein_seq = re.sub(r'\\s+', '', protein_seq.upper())\n",
    "    if not protein_seq.strip():\n",
    "        return True, \"\", {}, protein_seq\n",
    "\n",
    "    # Check for invalid characters (only uppercase letters and : allowed)\n",
    "    invalid_chars = re.findall(r'[^A-Z:]', protein_seq)\n",
    "    if invalid_chars:\n",
    "        invalid_chars_str = ', '.join(set(invalid_chars))\n",
    "        return False, f\"Invalid characters found: {invalid_chars_str}. Only uppercase letters (A-Z) and colon (:) are allowed.\", {}, protein_seq\n",
    "\n",
    "    # Split by colon to get chains\n",
    "    chains = protein_seq.split(':')\n",
    "\n",
    "    # Check if we have too many chains (max 25 chains: A-W, Y-Z, X is reserved for ligand)\n",
    "    if len(chains) > 25:\n",
    "        return False, f\"Too many protein chains ({len(chains)}). Maximum allowed is 25 chains (A-W, Y-Z). Chain X is reserved for ligands.\", {}, protein_seq\n",
    "\n",
    "    # Validate each chain contains only uppercase letters\n",
    "    for i, chain in enumerate(chains):\n",
    "        if not chain.strip():\n",
    "            return False, f\"Empty chain found at position {i+1}. Each chain must contain amino acid sequence.\", {}, protein_seq\n",
    "        if not re.match(r'^[A-Z]+$', chain.strip()):\n",
    "            return False, f\"Chain {i+1} contains invalid characters. Only uppercase letters are allowed.\", {}, protein_seq\n",
    "\n",
    "    # Create chains dictionary with incremental IDs (A-W, Y-Z, X reserved for ligand)\n",
    "    chains_dict = {}\n",
    "    for i, chain in enumerate(chains):\n",
    "        if i < 23:  # A-W (0-22)\n",
    "            chain_id = chr(65 + i)  # A, B, C, ..., W (65-87)\n",
    "        else:  # Y-Z (23-24)\n",
    "            chain_id = chr(89 + (i - 23))  # Y, Z (89-90)\n",
    "        chains_dict[chain_id] = chain.strip()\n",
    "\n",
    "    return True, \"\", chains_dict, protein_seq\n",
    "\n",
    "def parse_smiles_list_colab(smiles_input: str):\n",
    "    \"\"\"\n",
    "    Parse SMILES input that can be either semicolon-separated or newline-separated.\n",
    "    Optimized for Google Colab where semicolons are preferred.\n",
    "    \"\"\"\n",
    "    smiles_list = []\n",
    "\n",
    "    # Handle both semicolon and newline separation\n",
    "    if ';' in smiles_input:\n",
    "        # Semicolon-separated (preferred for Colab)\n",
    "        lines = smiles_input.strip().split(';')\n",
    "    else:\n",
    "        # Fallback to newline-separated\n",
    "        lines = smiles_input.strip().split('\\n')\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if line contains comma (name, smiles format)\n",
    "        if ',' in line:\n",
    "            parts = line.split(',', 1)  # Split only on first comma\n",
    "            if len(parts) == 2:\n",
    "                name = parts[0].strip()\n",
    "                smiles = parts[1].strip()\n",
    "                smiles_list.append((name, smiles))\n",
    "            else:\n",
    "                smiles_list.append((f\"Compound_{i+1}\", line))\n",
    "        else:\n",
    "            # Just SMILES without name\n",
    "            smiles_list.append((f\"Compound_{i+1}\", line))\n",
    "\n",
    "    return smiles_list\n",
    "\n",
    "# Validate inputs\n",
    "print(\"ðŸ” Validating Basic Inputs...\\n\")\n",
    "\n",
    "# Handle mutation discovery if requested\n",
    "discovered_mutations = []\n",
    "detailed_mutations_data = []\n",
    "if input_method == \"Mutants-Drug Screening\" and query_mutations_from_database and wt_protein_sequence.strip():\n",
    "    print(\"ðŸ” Discovering mutations from public databases...\\n\")\n",
    "\n",
    "    # Clean and validate the sequence first\n",
    "    is_valid, error_msg, chains_dict, clean_seq = validate_protein_sequence(wt_protein_sequence.strip())\n",
    "\n",
    "    if not is_valid:\n",
    "        print(f\"âŒ Invalid wild-type sequence: {error_msg}\")\n",
    "    else:\n",
    "        # Parse chain start numbers\n",
    "        num_chains = len(chains_dict) if chains_dict else 1\n",
    "        chain_starts = parse_chain_start_numbers(chain_start_numbers, num_chains)\n",
    "\n",
    "        # Parse residue range filter\n",
    "        range_filter = parse_residue_range_filter(residue_range_filter, num_chains)\n",
    "\n",
    "        print(f\"ðŸ§¬ Multi-chain analysis:\")\n",
    "        print(f\"   Chains detected: {list(chains_dict.keys()) if chains_dict else ['A (single chain)']}\")\n",
    "        print(f\"   Chain start numbers: {chain_starts}\")\n",
    "        if range_filter:\n",
    "            print(f\"   Residue range filter: {range_filter}\")\n",
    "\n",
    "        # Use first/longest chain for identification\n",
    "        if chains_dict and len(chains_dict) > 1:\n",
    "            # Use the longest chain for protein identification\n",
    "            first_chain = max(chains_dict.values(), key=len)\n",
    "            chain_id_for_analysis = max(chains_dict.items(), key=lambda x: len(x[1]))[0]\n",
    "            print(f\"   Using chain {chain_id_for_analysis} (longest, {len(first_chain)} residues) for protein identification\")\n",
    "        else:\n",
    "            first_chain = clean_seq.replace(':', '') if ':' in clean_seq else clean_seq\n",
    "            chain_id_for_analysis = 'A'\n",
    "            print(f\"   Single chain analysis ({len(first_chain)} residues)\")\n",
    "\n",
    "        print(f\"\\nðŸ” Analyzing sequence (length: {len(first_chain)} residues)...\")\n",
    "\n",
    "        # Identify protein\n",
    "        protein_info = identify_protein_from_sequence(first_chain)\n",
    "\n",
    "        if \"error\" not in protein_info:\n",
    "            print(f\"âœ… Protein identified:\")\n",
    "            print(f\"   ðŸ“› Name: {protein_info.get('protein_name', 'Unknown')}\")\n",
    "            print(f\"   ðŸ§¬ Organism: {protein_info.get('organism', 'Unknown')}\")\n",
    "            print(f\"   ðŸ”— UniProt: {protein_info.get('accession', 'Unknown')}\")\n",
    "            print(f\"   ðŸ“Š Similarity: {protein_info.get('similarity', 0):.1%}\")\n",
    "\n",
    "            print(f\"\\nðŸ” Searching for known mutations...\")\n",
    "\n",
    "            # Use the chain start number for the analysis chain\n",
    "            analysis_chain_start = chain_starts.get(chain_id_for_analysis, 1)\n",
    "\n",
    "            # Use the detailed mutation query function\n",
    "            detailed_mutations_data = query_mutations_for_protein_detailed(\n",
    "                protein_info, first_chain, analysis_chain_start, 50  # Increased default to show more mutations\n",
    "            )\n",
    "\n",
    "            if detailed_mutations_data:\n",
    "                # Apply residue range filter if specified\n",
    "                if range_filter:\n",
    "                    print(f\"   ðŸŽ¯ Applying residue range filter...\")\n",
    "                    original_count = len(detailed_mutations_data)\n",
    "                    detailed_mutations_data = filter_mutations_by_range(detailed_mutations_data, range_filter, chain_starts)\n",
    "                    filtered_count = len(detailed_mutations_data)\n",
    "                    print(f\"   ðŸ“Š Filtered {original_count} â†’ {filtered_count} mutations based on range criteria\")\n",
    "\n",
    "                discovered_mutations = [m['mutation_code'] for m in detailed_mutations_data]\n",
    "                formatted_mutations = format_mutations_for_input(detailed_mutations_data)\n",
    "\n",
    "                print(f\"âœ… Found {len(detailed_mutations_data)} mutations:\")\n",
    "\n",
    "                # Display detailed table with improved format (no redundant columns)\n",
    "                print(f\"\\nðŸ“Š Detailed Mutation Analysis:\")\n",
    "                mutation_table = format_mutations_table_improved(detailed_mutations_data)\n",
    "                print(mutation_table)\n",
    "\n",
    "                print(f\"\\nðŸ“‹ Copy this to the mutations input field below:\")\n",
    "                print(f\"ðŸ”— {formatted_mutations}\")\n",
    "\n",
    "                print(f\"\\nðŸ“š Column Explanations:\")\n",
    "                print(f\"â€¢ Mutation: Mutation in your sequence numbering (WT_AA + Position + MUT_AA)\")\n",
    "                print(f\"â€¢ Change Type: Chemical property change classification\")\n",
    "                print(f\"â€¢ Severity: Predicted functional impact (High/Moderate/Low)\")\n",
    "                print(f\"â€¢ Structural Impact: Expected structural consequences\")\n",
    "                print(f\"â€¢ Evidence: Number of experimental observations in databases\")\n",
    "\n",
    "                if range_filter:\n",
    "                    print(f\"\\nðŸŽ¯ Range Filter Applied:\")\n",
    "                    for chain_id, ranges in range_filter.items():\n",
    "                        for start_pos, end_pos in ranges:\n",
    "                            actual_start = start_pos + chain_starts[chain_id] - 1\n",
    "                            actual_end = end_pos + chain_starts[chain_id] - 1\n",
    "                            print(f\"   Chain {chain_id}: residues {start_pos}-{end_pos} (absolute positions {actual_start}-{actual_end})\")\n",
    "\n",
    "            else:\n",
    "                print(f\"âš ï¸ No mutations found in the specified criteria.\")\n",
    "        else:\n",
    "            print(f\"âŒ Protein identification failed: {protein_info['error']}\")\n",
    "            print(f\"ðŸ’¡ You can still manually enter mutations in the standard format (e.g., A50V, L60P)\")\n",
    "\n",
    "# Handle file upload if selected\n",
    "protein_sequences = []\n",
    "if input_method == \"Protein-Drug Screening - Upload FASTA File\":\n",
    "    if COLAB_AVAILABLE and not running_locally:\n",
    "        print(\"ðŸ“ Please upload your FASTA file:\")\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        for filename, content in uploaded.items():\n",
    "            if isinstance(content, bytes):\n",
    "                content = content.decode('utf-8')\n",
    "\n",
    "            sequences = parse_fasta_sequences(content)\n",
    "            protein_sequences.extend(sequences)\n",
    "            print(f\"âœ… Parsed {len(sequences)} sequences from {filename}\")\n",
    "\n",
    "            for name, seq in sequences:\n",
    "                print(f\"  - {name}: {len(seq)} residues\")\n",
    "    elif running_locally:\n",
    "        print(\"ðŸ“ Local mode: Looking for protein.fasta in current directory...\")\n",
    "        sequences = parse_fasta_from_local_file(\"protein.fasta\")\n",
    "        if sequences:\n",
    "            protein_sequences.extend(sequences)\n",
    "            print(f\"âœ… Parsed {len(sequences)} sequences from protein.fasta\")\n",
    "            for name, seq in sequences:\n",
    "                print(f\"  - {name}: {len(seq)} residues\")\n",
    "        else:\n",
    "            print(\"âŒ protein.fasta not found or empty in current directory\")\n",
    "    else:\n",
    "        print(\"âŒ Cannot upload files - not in Colab environment and not in local mode\")\n",
    "\n",
    "elif input_method == \"Protein-Drug Screening - Manual Protein Entry\":\n",
    "    # Validate manual protein input using colon-separated format\n",
    "    is_valid, error_msg, chains_dict, clean_seq = validate_protein_sequence(protein_sequence)\n",
    "    if is_valid:\n",
    "        protein_sequences.append((protein_name, protein_sequence))\n",
    "        print(f\"âœ… Valid protein sequence: {protein_name}\")\n",
    "        if len(chains_dict) > 1:\n",
    "            print(f\"   Multi-chain detected: {len(chains_dict)} chains ({list(chains_dict.keys())})\")\n",
    "            for chain_id, chain_seq in chains_dict.items():\n",
    "                print(f\"     Chain {chain_id}: {len(chain_seq)} residues\")\n",
    "        else:\n",
    "            # Single chain\n",
    "            clean_single_seq = re.sub(r'\\s+', '', protein_sequence.upper())\n",
    "            print(f\"   Length: {len(clean_single_seq)} residues\")\n",
    "    else:\n",
    "        print(f\"âŒ Invalid protein sequence: {error_msg}\")\n",
    "        protein_sequences = []\n",
    "\n",
    "elif input_method == \"Mutants-Drug Screening\":\n",
    "    # Process mutation mode\n",
    "    if wt_protein_sequence.strip() and mutations_input.strip():\n",
    "        is_valid, error_msg, chains_dict, clean_seq = validate_protein_sequence(wt_protein_sequence.strip())\n",
    "\n",
    "        if is_valid:\n",
    "            # Parse chain start numbers\n",
    "            num_chains = len(chains_dict) if chains_dict else 1\n",
    "            chain_starts = parse_chain_start_numbers(chain_start_numbers, num_chains)\n",
    "\n",
    "            # Parse mutations\n",
    "            mutation_lists = parse_mutations(mutations_input)\n",
    "\n",
    "            # Create wild-type entry\n",
    "            protein_sequences.append((\"WT\", clean_seq))\n",
    "\n",
    "            # Create mutant entries\n",
    "            mutant_strings = [s.strip() for s in mutations_input.split(',')]\n",
    "\n",
    "            for i, mutant_str in enumerate(mutant_strings):\n",
    "                if not mutant_str:\n",
    "                    continue\n",
    "\n",
    "                mutant_name = generate_mutant_name(mutation_lists[i] if i < len(mutation_lists) else [])\n",
    "\n",
    "                if i < len(mutation_lists):\n",
    "                    mutations = mutation_lists[i]\n",
    "                    mutated_seq = apply_mutations_to_sequence(clean_seq, mutations, chain_starts, chains_dict)\n",
    "                    protein_sequences.append((mutant_name, mutated_seq))\n",
    "\n",
    "            print(f\"âœ… Generated {len(protein_sequences)} protein variants (WT + {len(protein_sequences)-1} mutants)\")\n",
    "\n",
    "            # Display chain information if multi-chain\n",
    "            if chains_dict and len(chains_dict) > 1:\n",
    "                print(f\"   Multi-chain setup: {len(chains_dict)} chains with start numbers {chain_starts}\")\n",
    "        else:\n",
    "            print(f\"âŒ Invalid wild-type sequence: {error_msg}\")\n",
    "\n",
    "# Validate SMILES input\n",
    "ligand_smiles = []\n",
    "if not structure_only:\n",
    "    if ligand_input_method == \"Protein-Drug Screening\":\n",
    "        if COLAB_AVAILABLE and not running_locally:\n",
    "            print(\"ðŸ“ Please upload your SMILES FASTA file:\")\n",
    "            uploaded_smiles = files.upload()\n",
    "\n",
    "            for filename, content in uploaded_smiles.items():\n",
    "                if isinstance(content, bytes):\n",
    "                    content = content.decode('utf-8')\n",
    "\n",
    "                parsed_smiles = parse_smiles_list(content)\n",
    "                print(f\"âœ… Parsed {len(parsed_smiles)} SMILES from {filename}\")\n",
    "                ligand_smiles.extend(parsed_smiles)\n",
    "        elif running_locally:\n",
    "            print(\"ðŸ“ Local mode: Looking for drug.fasta in current directory...\")\n",
    "            sequences = parse_fasta_from_local_file(\"drug.fasta\")\n",
    "            if sequences:\n",
    "                # Convert FASTA sequences to SMILES format\n",
    "                parsed_smiles = [(name, seq) for name, seq in sequences]\n",
    "                ligand_smiles.extend(parsed_smiles)\n",
    "                print(f\"âœ… Parsed {len(parsed_smiles)} SMILES from drug.fasta\")\n",
    "            else:\n",
    "                print(\"âŒ drug.fasta not found or empty in current directory\")\n",
    "        else:\n",
    "            print(\"âŒ Cannot upload files - not in Colab environment and not in local mode\")\n",
    "\n",
    "    elif ligand_input_method == \"Text Input\" and ligand_smiles_input.strip():\n",
    "        parsed_smiles = parse_smiles_list_colab(ligand_smiles_input)\n",
    "        print(f\"\\nðŸ§ª Validating {len(parsed_smiles)} SMILES strings:\")\n",
    "\n",
    "        for name, smiles in parsed_smiles:\n",
    "            is_valid, error_msg = validate_smiles(smiles)\n",
    "            if is_valid:\n",
    "                ligand_smiles.append((name, smiles))\n",
    "                print(f\"âœ… {name}: {smiles}\")\n",
    "                if RDKIT_AVAILABLE and \"Valid (\" in error_msg:\n",
    "                    print(f\"   {error_msg}\")\n",
    "            else:\n",
    "                print(f\"âŒ {name}: {error_msg}\")\n",
    "\n",
    "# Store validated inputs for next cell\n",
    "print(f\"\\nðŸ“‹ Basic Input Summary:\")\n",
    "print(f\"ðŸ“¦ Project: {project_name}\")\n",
    "print(f\"ðŸ§¬ Valid proteins: {len(protein_sequences)}\")\n",
    "print(f\"ðŸ§ª Valid ligands: {len(ligand_smiles) if not structure_only else 'N/A (structure only)'}\")\n",
    "\n",
    "if discovered_mutations:\n",
    "    print(f\"ðŸ” Discovered mutations: {len(discovered_mutations)}\")\n",
    "    if detailed_mutations_data:\n",
    "        severity_counts = {}\n",
    "        for mut in detailed_mutations_data:\n",
    "            severity = mut.get('severity', 'Unknown')\n",
    "            severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "\n",
    "        severity_summary = \", \".join([f\"{count} {sev}\" for sev, count in severity_counts.items()])\n",
    "        print(f\"   Severity breakdown: {severity_summary}\")\n",
    "\n",
    "if protein_sequences and (ligand_smiles or structure_only):\n",
    "    print(f\"\\nâœ… Basic inputs validated! Proceed to the next cell for additional parameters.\")\n",
    "else:\n",
    "    if structure_only:\n",
    "        needed = \"at least 1 protein\"\n",
    "    else:\n",
    "        needed = \"at least 1 protein and 1 ligand\"\n",
    "    print(f\"\\nâŒ Cannot proceed: Need {needed}\")"
   ]
  },
  {
   "metadata": {
    "id": "_mnsNhDkW_Pp"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title 3ï¸âƒ£ âš™ï¸ Additional Parameters (modifications are optional, run to proceed to next step) { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## ðŸ’Ž Co-factors (Optional)\n",
    "#@markdown Add essential co-factors like ATP, NAD, or heme that are required for protein function. Leave empty if not needed.\n",
    "\n",
    "#@markdown ### ðŸ”¸ Co-factor 1\n",
    "#@markdown **Method**: 'SMILES' = provide chemical structure, 'CCD Code' = use standard biochemical codes (ATP, NAD, HEM, etc.).\n",
    "cofactor1_method = \"None\"  #@param [\"None\", \"SMILES\", \"CCD Code\"]\n",
    "#@markdown **SMILES**: Chemical structure notation (only if SMILES method selected)\n",
    "cofactor1_smiles = \"\"  #@param {type:\"string\"}\n",
    "#@markdown **CCD Code**: Standard biochemical code (only if CCD Code method selected)\n",
    "cofactor1_ccd = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Co-factor 2\n",
    "#@markdown **Method**: Choose input method for second co-factor\n",
    "cofactor2_method = \"None\"  #@param [\"None\", \"SMILES\", \"CCD Code\"]\n",
    "#@markdown **SMILES**: Chemical structure notation\n",
    "cofactor2_smiles = \"\"  #@param {type:\"string\"}\n",
    "#@markdown **CCD Code**: Standard biochemical code\n",
    "cofactor2_ccd = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Co-factor 3\n",
    "#@markdown **Method**: Choose input method for third co-factor\n",
    "cofactor3_method = \"None\"  #@param [\"None\", \"SMILES\", \"CCD Code\"]\n",
    "#@markdown **SMILES**: Chemical structure notation\n",
    "cofactor3_smiles = \"\"  #@param {type:\"string\"}\n",
    "#@markdown **CCD Code**: Standard biochemical code\n",
    "cofactor3_ccd = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Co-factor 4\n",
    "#@markdown **Method**: Choose input method for fourth co-factor\n",
    "cofactor4_method = \"None\"  #@param [\"None\", \"SMILES\", \"CCD Code\"]\n",
    "#@markdown **SMILES**: Chemical structure notation\n",
    "cofactor4_smiles = \"\"  #@param {type:\"string\"}\n",
    "#@markdown **CCD Code**: Standard biochemical code\n",
    "cofactor4_ccd = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ## ðŸŽ¯ Binding Pocket Constraints (Optional)\n",
    "#@markdown Define specific residues that should interact with ligands. Use for targeted drug design.\n",
    "\n",
    "#@markdown **Enable constraints**: Force ligands to bind near specific protein residues.\n",
    "enable_binding_constraints = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Binder ID**: Which molecule should satisfy constraints ('X' = main ligand).\n",
    "binder_id = \"X\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Contact residues**: Comma-separated list (format: 'A:25,A:48,B:52' = Chain A residue 25, Chain A residue 48, Chain B residue 52).\n",
    "pocket_contacts = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Max distance**: Maximum allowed distance in Angstroms between ligand and specified residues.\n",
    "max_distance = 5.0  #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ## âš—ï¸ Post-Translational Modifications - PTM (Optional)\n",
    "#@markdown Add chemical modifications to proteins (phosphorylation, glycosylation, etc.).\n",
    "\n",
    "#@markdown **Enable PTM**: Add post-translational modifications to specific residues.\n",
    "enable_ptm = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **PTM chain**: Which protein chain to modify (usually 'A' for single chains).\n",
    "ptm_chain_id = \"A\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **PTM position**: Residue number where modification should be added.\n",
    "ptm_position = 100  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **PTM type**: Chemical modification code (e.g., 'PLP' for phosphorylation, 'NAG' for glycosylation).\n",
    "ptm_ccd_code = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown Click RUN â–¶ï¸ to validate additional parameters. Then proceed to the next cell for computational settings and generation.\n",
    "\n",
    "# Check if basic inputs were validated\n",
    "if 'protein_sequences' not in globals() or not protein_sequences:\n",
    "    print(\"âŒ Please run the Basic Input Configuration cell first!\")\n",
    "else:\n",
    "    print(\"ðŸ” Validating Additional Parameters...\\n\")\n",
    "\n",
    "    # Process co-factors automatically based on user input\n",
    "    cofactor_info = []\n",
    "    cofactor_configs = [\n",
    "        (cofactor1_method, cofactor1_smiles, cofactor1_ccd),\n",
    "        (cofactor2_method, cofactor2_smiles, cofactor2_ccd),\n",
    "        (cofactor3_method, cofactor3_smiles, cofactor3_ccd),\n",
    "        (cofactor4_method, cofactor4_smiles, cofactor4_ccd)\n",
    "    ]\n",
    "\n",
    "    valid_cofactors = 0\n",
    "    for i, (method, smiles_val, ccd_val) in enumerate(cofactor_configs):\n",
    "        if method == \"None\":\n",
    "            continue\n",
    "\n",
    "        if method == \"SMILES\" and smiles_val.strip():\n",
    "            is_valid, _ = validate_smiles(smiles_val.strip())\n",
    "            if is_valid:\n",
    "                cofactor_info.append({'smiles': smiles_val.strip()})\n",
    "                valid_cofactors += 1\n",
    "                print(f\"âœ… Co-factor {valid_cofactors}: SMILES - {smiles_val.strip()}\")\n",
    "            else:\n",
    "                print(f\"âŒ Co-factor {i+1}: Invalid SMILES - {smiles_val.strip()}\")\n",
    "        elif method == \"CCD Code\" and ccd_val.strip():\n",
    "            if validate_ccd_code(ccd_val.strip()):\n",
    "                cofactor_info.append({'ccd': ccd_val.strip().upper()})\n",
    "                ccd_name = COMMON_CCD_CODES.get(ccd_val.strip().upper(), \"Unknown\")\n",
    "                valid_cofactors += 1\n",
    "                print(f\"âœ… Co-factor {valid_cofactors}: {ccd_val.strip().upper()} - {ccd_name}\")\n",
    "            else:\n",
    "                print(f\"âŒ Co-factor {i+1}: Invalid CCD code - {ccd_val.strip()}\")\n",
    "\n",
    "    if valid_cofactors > 0:\n",
    "        print(f\"\\nðŸ’Ž Total valid co-factors: {valid_cofactors}\")\n",
    "\n",
    "    # Process binding pocket constraints\n",
    "    binding_pocket_constraints = None\n",
    "    if enable_binding_constraints and pocket_contacts.strip():\n",
    "        print(f\"\\nðŸŽ¯ Processing binding pocket constraints:\")\n",
    "        contacts = []\n",
    "        for contact_str in pocket_contacts.split(','):\n",
    "            contact_str = contact_str.strip()\n",
    "            if ':' in contact_str:\n",
    "                chain_id, pos_str = contact_str.split(':', 1)\n",
    "                try:\n",
    "                    position = int(pos_str.strip())\n",
    "                    contacts.append([chain_id.strip(), position])\n",
    "                    print(f\"âœ… Contact: Chain {chain_id.strip()} position {position}\")\n",
    "                except ValueError:\n",
    "                    print(f\"âŒ Invalid contact format: {contact_str}\")\n",
    "            else:\n",
    "                try:\n",
    "                    position = int(contact_str)\n",
    "                    contacts.append(['A', position])\n",
    "                    print(f\"âœ… Contact: Chain A position {position}\")\n",
    "                except ValueError:\n",
    "                    print(f\"âŒ Invalid contact format: {contact_str}\")\n",
    "\n",
    "        if contacts:\n",
    "            binding_pocket_constraints = {\n",
    "                'binder': binder_id,\n",
    "                'contacts': contacts,\n",
    "                'max_distance': max_distance\n",
    "            }\n",
    "\n",
    "    # Process PTM modifications\n",
    "    ptm_modifications = None\n",
    "    if enable_ptm and ptm_ccd_code.strip():\n",
    "        print(f\"\\nâš—ï¸ Processing PTM modifications:\")\n",
    "        if validate_ccd_code(ptm_ccd_code.strip()):\n",
    "            ptm_modifications = {\n",
    "                'modifications': [{\n",
    "                    'chain_id': ptm_chain_id,\n",
    "                    'position': ptm_position,\n",
    "                    'ccd': ptm_ccd_code.strip().upper()\n",
    "                }]\n",
    "            }\n",
    "            ptm_name = COMMON_CCD_CODES.get(ptm_ccd_code.strip().upper(), \"Unknown\")\n",
    "            print(f\"âœ… PTM: {ptm_ccd_code.strip().upper()} at Chain {ptm_chain_id} position {ptm_position} - {ptm_name}\")\n",
    "        else:\n",
    "            print(f\"âŒ Invalid PTM CCD code: {ptm_ccd_code.strip()}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nðŸ“‹ Complete Input Summary:\")\n",
    "    print(f\"ðŸ“¦ Project: {project_name}\")\n",
    "    print(f\"ðŸ§¬ Valid proteins: {len(protein_sequences)}\")\n",
    "    print(f\"ðŸ§ª Valid ligands: {len(ligand_smiles) if not structure_only else 'N/A (structure only)'}\")\n",
    "    print(f\"ðŸ’Ž Co-factors: {len(cofactor_info)}\")\n",
    "    print(f\"ðŸŽ¯ Binding constraints: {'Yes' if binding_pocket_constraints else 'No'}\")\n",
    "    print(f\"âš—ï¸ PTM modifications: {'Yes' if ptm_modifications else 'No'}\")\n",
    "\n",
    "    if protein_sequences and (ligand_smiles or structure_only):\n",
    "        total_combinations = len(protein_sequences) * (len(ligand_smiles) if not structure_only else 1)\n",
    "        estimated_time = total_combinations * ESTIMATED_TIME_PER_JOB / 60\n",
    "        print(f\"ðŸ”¢ Total combinations: {total_combinations}\")\n",
    "        print(f\"â±ï¸ Estimated time: {estimated_time:.1f} minutes\")\n",
    "        print(f\"\\nâœ… All parameters validated! Proceed to the next cell to configure and generate screening.\")\n",
    "    else:\n",
    "        if structure_only:\n",
    "            needed = \"at least 1 protein\"\n",
    "        else:\n",
    "            needed = \"at least 1 protein and 1 ligand\"\n",
    "        print(f\"\\nâŒ Cannot proceed: Need {needed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_predictions"
   },
   "outputs": [],
   "source": [
    "#@title 4ï¸âƒ£ ðŸš€ Configure Computational Settings & Generate Screening (modifications are optional, run to proceed to next step) { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## âš™ï¸ Computation Settings\n",
    "#@markdown Configure the computational parameters for structure prediction and analysis.\n",
    "\n",
    "#@markdown ### ðŸ”¸ Basic Settings\n",
    "\n",
    "#@markdown **GPU acceleration**: Enable for faster computation (~10x speedup). Disable only if GPU unavailable.\n",
    "use_gpu = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Use existing results**: Load previously computed results to save time. Disable to force fresh computation.\n",
    "use_existing_results = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Parallel samples**: Number of simultaneous predictions. Higher values use more memory but run faster.\n",
    "max_parallel_samples = 5  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Advanced Sampling Parameters\n",
    "\n",
    "#@markdown **Recycling steps**: Number of iterative refinement cycles (1-10). More steps = higher accuracy, longer time.\n",
    "recycling_steps = 4  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Sampling steps**: Diffusion sampling iterations (50-500). More steps = better quality, longer computation.\n",
    "sampling_steps = 300  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Diffusion samples**: Multiple structure samples per prediction (1-5). More samples = better statistics.\n",
    "diffusion_samples = 1  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Step scale**: Controls sampling temperature (1.0-2.0). Lower = more diversity, higher = more precision.\n",
    "step_scale = 1.638  #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Multiple Sequence Alignment\n",
    "\n",
    "#@markdown **MSA sequences**: Maximum evolutionary sequences to use (1024-16384). More sequences = better accuracy.\n",
    "max_msa_seqs = 8192  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Subsample MSA**: Randomly reduce MSA size to increase diversity while saving memory.\n",
    "subsample_msa = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Subsampled count**: Number of sequences when subsampling (only if subsampling enabled).\n",
    "num_subsampled_msa = 1024  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Affinity Prediction\n",
    "\n",
    "#@markdown **Molecular weight correction**: Apply size-based correction to binding affinity predictions.\n",
    "affinity_mw_correction = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Affinity sampling steps**: Dedicated steps for binding affinity calculation.\n",
    "sampling_steps_affinity = 300  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Affinity samples**: Multiple samples for robust affinity estimation.\n",
    "diffusion_samples_affinity = 7  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### ðŸ”¸ Error Handling\n",
    "\n",
    "#@markdown **Enable retries**: Automatically retry failed predictions with exponential backoff delay.\n",
    "enable_retries = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Max retries**: Maximum retry attempts per failed prediction (1-5).\n",
    "max_retry_attempts = 2  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Retry delay**: Base delay in seconds between retries (doubles each attempt).\n",
    "retry_delay_base = 5  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ## ðŸ“‹ Template Upload (Optional)\n",
    "#@markdown **Upload template structure**: Provide a .cif template file to guide protein folding (advanced users only). Local mode: looks for template.cif in working directory.\n",
    "upload_template = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown Click RUN â–¶ï¸ to generate comprehensive screening configurations.\n",
    "\n",
    "# Check prerequisites\n",
    "if 'protein_sequences' not in globals() or 'ligand_smiles' not in globals():\n",
    "    print(\"âŒ Please run the previous configuration cells first!\")\n",
    "elif not protein_sequences:\n",
    "    print(\"âŒ No valid protein sequences found. Check your inputs.\")\n",
    "elif not structure_only and not ligand_smiles:\n",
    "    print(\"âŒ No valid ligands found and not in structure-only mode. Check your inputs.\")\n",
    "else:\n",
    "    print(f\"ðŸ”„ Generating Comprehensive Screening Configurations\\n\")\n",
    "    print(f\"ðŸ“¦ Project: {project_name}\")\n",
    "    print(f\"ðŸ§¬ Proteins: {len(protein_sequences)}\")\n",
    "    print(f\"ðŸ§ª Ligands: {len(ligand_smiles) if not structure_only else 'N/A (structure only)'}\")\n",
    "    print(f\"ðŸŸ¢ Co-factors: {len(cofactor_info) if 'cofactor_info' in globals() else 0}\")\n",
    "    print(f\"ðŸŽ¯ Binding constraints: {'Yes' if 'binding_pocket_constraints' in globals() and binding_pocket_constraints else 'No'}\")\n",
    "    print(f\"ðŸ§ª PTM modifications: {'Yes' if 'ptm_modifications' in globals() and ptm_modifications else 'No'}\")\n",
    "\n",
    "    # Handle template file upload\n",
    "    template_cif_path = None\n",
    "    if upload_template and COLAB_AVAILABLE:\n",
    "        print(f\"\\nðŸ“ Template File Upload:\")\n",
    "        uploaded_template = files.upload()\n",
    "\n",
    "        for filename, content in uploaded_template.items():\n",
    "            if filename.endswith('.cif'):\n",
    "                # Save template file\n",
    "                project_dir = os.path.join(RESULTS_DIR, project_name)\n",
    "                os.makedirs(project_dir, exist_ok=True)\n",
    "                cif_filename = f\"template_{datetime.now().strftime('%Y%m%d_%H%M%S')}.cif\"\n",
    "                template_cif_path = os.path.join(project_dir, cif_filename)\n",
    "                with open(template_cif_path, \"wb\") as f:\n",
    "                    f.write(content)\n",
    "                template_cif_path = os.path.abspath(template_cif_path)\n",
    "                print(f\"âœ… Template saved: {cif_filename}\")\n",
    "            else:\n",
    "                print(f\"âŒ Unsupported file type: {filename} (only .cif supported)\")\n",
    "\n",
    "    # Store configuration for next cells\n",
    "    screening_config = {\n",
    "        'use_gpu': use_gpu,\n",
    "        'recycling_steps': recycling_steps,\n",
    "        'sampling_steps': sampling_steps,\n",
    "        'diffusion_samples': diffusion_samples,\n",
    "        'step_scale': step_scale,\n",
    "        'max_msa_seqs': max_msa_seqs,\n",
    "        'subsample_msa': subsample_msa,\n",
    "        'num_subsampled_msa': num_subsampled_msa if subsample_msa else None,\n",
    "        'affinity_mw_correction': affinity_mw_correction,\n",
    "        'sampling_steps_affinity': sampling_steps_affinity,\n",
    "        'diffusion_samples_affinity': diffusion_samples_affinity,\n",
    "        'enable_retries': enable_retries,\n",
    "        'max_retry_attempts': max_retry_attempts,\n",
    "        'retry_delay_base': retry_delay_base,\n",
    "        'use_existing_results': use_existing_results,\n",
    "        'max_parallel_samples': max_parallel_samples\n",
    "    }\n",
    "\n",
    "    # Prepare configuration with all advanced parameters\n",
    "    advanced_config = screening_config.copy()\n",
    "\n",
    "    # Add optional components\n",
    "    if 'cofactor_info' in globals():\n",
    "        advanced_config['cofactor_info'] = cofactor_info\n",
    "    if 'binding_pocket_constraints' in globals():\n",
    "        advanced_config['binding_pocket_constraints'] = binding_pocket_constraints\n",
    "    if 'ptm_modifications' in globals():\n",
    "        advanced_config['ptm_modifications'] = ptm_modifications\n",
    "    if template_cif_path:\n",
    "        advanced_config['template_cif_path'] = template_cif_path\n",
    "\n",
    "    advanced_config['structure_only'] = structure_only\n",
    "\n",
    "    print(f\"\\nâš™ï¸ Configuration Summary:\")\n",
    "    print(f\"ðŸ”§ GPU: {advanced_config['use_gpu']}\")\n",
    "    print(f\"ðŸ”„ Recycling steps: {advanced_config['recycling_steps']}\")\n",
    "    print(f\"ðŸ“Š Sampling steps: {advanced_config['sampling_steps']}\")\n",
    "    print(f\"ðŸŒŠ Diffusion samples: {advanced_config['diffusion_samples']}\")\n",
    "    print(f\"âš¡ Step scale: {advanced_config['step_scale']}\")\n",
    "    print(f\"ðŸ§¬ MSA sequences: {advanced_config['max_msa_seqs']}\")\n",
    "    if advanced_config.get('subsample_msa'):\n",
    "        print(f\"ðŸ“‰ MSA subsampling: {advanced_config['num_subsampled_msa']} sequences\")\n",
    "    if advanced_config.get('affinity_mw_correction'):\n",
    "        print(f\"âš—ï¸ Affinity MW correction: Enabled\")\n",
    "    if advanced_config.get('enable_retries'):\n",
    "        print(f\"ðŸ” Retry attempts: {advanced_config['max_retry_attempts']}\")\n",
    "\n",
    "    # Create YAML files for each combination\n",
    "    yaml_files = []\n",
    "    print(f\"\\nðŸ“„ Creating Configuration Files:\")\n",
    "\n",
    "    # Structure-only mode or ligand mode\n",
    "    if structure_only:\n",
    "        combinations = [(prot_name, prot_seq, None, None) for prot_name, prot_seq in protein_sequences]\n",
    "    else:\n",
    "        combinations = [(prot_name, prot_seq, lig_name, lig_smiles)\n",
    "                       for prot_name, prot_seq in protein_sequences\n",
    "                       for lig_name, lig_smiles in ligand_smiles]\n",
    "\n",
    "    for i, (prot_name, prot_seq, lig_name, lig_smiles) in enumerate(combinations):\n",
    "        workspace_name = f\"screening_{i+1:03d}\"\n",
    "        if structure_only:\n",
    "            design_name = f\"{prot_name}\".replace(' ', '_')\n",
    "        else:\n",
    "            design_name = f\"{prot_name}_{lig_name}\".replace(' ', '_')\n",
    "\n",
    "        try:\n",
    "            yaml_path = create_screening_yaml(\n",
    "                workspace_name=workspace_name,\n",
    "                design_name=design_name,\n",
    "                protein_sequence=prot_seq,\n",
    "                ligand_smiles=lig_smiles or \"\",  # Empty for structure-only\n",
    "                project_name=project_name,\n",
    "                **advanced_config\n",
    "            )\n",
    "            yaml_files.append(yaml_path)\n",
    "\n",
    "            # Show progress with details\n",
    "            config_details = []\n",
    "            if advanced_config.get('cofactor_info'):\n",
    "                config_details.append(f\"{len(advanced_config['cofactor_info'])} co-factors\")\n",
    "            if advanced_config.get('binding_pocket_constraints'):\n",
    "                constraint_count = len(advanced_config['binding_pocket_constraints'].get('contacts', []))\n",
    "                config_details.append(f\"{constraint_count} pocket constraints\")\n",
    "            if advanced_config.get('ptm_modifications'):\n",
    "                ptm_count = len(advanced_config['ptm_modifications'].get('modifications', []))\n",
    "                config_details.append(f\"{ptm_count} PTM modifications\")\n",
    "            if template_cif_path:\n",
    "                config_details.append(\"template structure\")\n",
    "\n",
    "            details_str = f\" ({', '.join(config_details)})\" if config_details else \"\"\n",
    "            print(f\"âœ… {design_name}{details_str}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error creating {design_name}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nðŸ“ Configuration Complete!\")\n",
    "    print(f\"ðŸ“„ Generated {len(yaml_files)} YAML files\")\n",
    "    print(f\"ðŸ“‚ Location: {RESULTS_DIR}/{project_name}/\")\n",
    "\n",
    "    # Display configuration file preview if any were created\n",
    "    if yaml_files:\n",
    "        print(f\"\\nðŸ” Sample Configuration Preview (first file):\")\n",
    "        try:\n",
    "            with open(yaml_files[0], 'r') as f:\n",
    "                preview_content = f.read()\n",
    "                # Show first 20 lines\n",
    "                preview_lines = preview_content.split('\\n')[:20]\n",
    "                print(\"```yaml\")\n",
    "                print('\\n'.join(preview_lines))\n",
    "                if len(preview_content.split('\\n')) > 20:\n",
    "                    print(\"... (truncated)\")\n",
    "                print(\"```\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading preview: {e}\")\n",
    "\n",
    "    # Store results for next steps\n",
    "    screening_yaml_files = yaml_files\n",
    "    screening_advanced_config = advanced_config\n",
    "\n",
    "    # Estimate computational requirements\n",
    "    total_combinations = len(yaml_files)\n",
    "    estimated_gpu_hours = total_combinations * (ESTIMATED_TIME_PER_JOB / 3600)  # Convert to hours\n",
    "    estimated_cost_estimate = estimated_gpu_hours * 0.50  # Rough estimate at $0.50/GPU hour\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title 5ï¸âƒ£ ðŸŒ  Run Structure Predictions { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown Click RUN â–¶ï¸ to proceed with structure predictions for all designs. This will execute the Boltz-2 prediction workflow using the configurations created in the previous steps.\n",
    "\n",
    "def run_boltz_prediction(yaml_filepath, use_gpu=True, override=False, recycling_steps=3, sampling_steps=200, diffusion_samples=1, max_parallel_samples=5, step_scale=1.638, affinity_mw_correction=False, max_msa_seqs=8192, sampling_steps_affinity=200, diffusion_samples_affinity=5, subsample_msa=False, num_subsampled_msa=1024):\n",
    "    \"\"\"Run Boltz prediction on the YAML file.\"\"\"\n",
    "    try:\n",
    "        # Change to the directory containing the YAML file\n",
    "        yaml_dir = os.path.dirname(yaml_filepath)\n",
    "        yaml_filename = os.path.basename(yaml_filepath)\n",
    "        # Run boltz predict command\n",
    "        cmd = [\"boltz\", \"predict\", yaml_filename, \"--use_msa_server\", \"--output_format\", \"pdb\"]\n",
    "        # Add override flag if specified\n",
    "        if override:\n",
    "            cmd.append(\"--override\")\n",
    "        # Add CPU accelerator flag if GPU is disabled\n",
    "        if not use_gpu:\n",
    "            cmd.append(\"--accelerator\")\n",
    "            cmd.append(\"cpu\")\n",
    "        # Add Boltz parameters\n",
    "        cmd.extend([\"--recycling_steps\", str(int(recycling_steps))])\n",
    "        cmd.extend([\"--sampling_steps\", str(int(sampling_steps))])\n",
    "        cmd.extend([\"--diffusion_samples\", str(int(diffusion_samples))])\n",
    "        cmd.extend([\"--max_parallel_samples\", str(int(max_parallel_samples))])\n",
    "        cmd.extend([\"--step_scale\", str(float(step_scale))])\n",
    "        if affinity_mw_correction:\n",
    "            cmd.append(\"--affinity_mw_correction\")\n",
    "        cmd.extend([\"--max_msa_seqs\", str(int(max_msa_seqs))])\n",
    "        cmd.extend([\"--sampling_steps_affinity\", str(int(sampling_steps_affinity))])\n",
    "        cmd.extend([\"--diffusion_samples_affinity\", str(int(diffusion_samples_affinity))])\n",
    "        # --- NEW: MSA Subsampling ---\n",
    "        if subsample_msa:\n",
    "            cmd.append(\"--subsample_msa\")\n",
    "            cmd.extend([\"--num_subsampled_msa\", str(int(num_subsampled_msa))])\n",
    "        # Print the command line for user reference\n",
    "        print(\"[DEBUG]\", \" \".join(cmd))\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            cwd=yaml_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # 5 minute timeout\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(f\"Boltz command failed: {result.stderr}\")\n",
    "        return result.stdout\n",
    "    except subprocess.TimeoutExpired:\n",
    "        raise Exception(\"Boltz prediction timed out after 5 minutes\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error running Boltz prediction: {str(e)}\")\n",
    "\n",
    "def parse_boltz_results(yaml_filepath, structure_only=False):\n",
    "    \"\"\"Parse Boltz results from the JSON output files.\"\"\"\n",
    "    try:\n",
    "        # Construct the path to the results files\n",
    "        yaml_dir = os.path.dirname(yaml_filepath)\n",
    "        yaml_filename = os.path.basename(yaml_filepath)\n",
    "        yaml_name = os.path.splitext(yaml_filename)[0]\n",
    "        \n",
    "        # Path to the affinity results JSON file - corrected path structure with underscores\n",
    "        affinity_results_path = os.path.join(yaml_dir, f\"boltz_results_{yaml_name}\", \"predictions\", yaml_name, f\"affinity_{yaml_name}.json\")\n",
    "        \n",
    "        # Path to the confidence results JSON file - corrected path structure with underscores\n",
    "        confidence_results_path = os.path.join(yaml_dir, f\"boltz_results_{yaml_name}\", \"predictions\", yaml_name, f\"confidence_{yaml_name}_model_0.json\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if not structure_only:\n",
    "            # Parse affinity results\n",
    "            if os.path.exists(affinity_results_path):\n",
    "                with open(affinity_results_path, 'r') as f:\n",
    "                    affinity_results = json.load(f)\n",
    "                # Extract the key values from affinity results\n",
    "                results.update({\n",
    "                    \"affinity_pred_value\": affinity_results.get(\"affinity_pred_value\"),\n",
    "                    \"affinity_probability_binary\": affinity_results.get(\"affinity_probability_binary\"),\n",
    "                    \"affinity_pred_value1\": affinity_results.get(\"affinity_pred_value1\"),\n",
    "                    \"affinity_probability_binary1\": affinity_results.get(\"affinity_probability_binary1\"),\n",
    "                    \"affinity_pred_value2\": affinity_results.get(\"affinity_pred_value2\"),\n",
    "                    \"affinity_probability_binary2\": affinity_results.get(\"affinity_probability_binary2\")\n",
    "                })\n",
    "            else:\n",
    "                raise Exception(f\"Affinity results file not found: {affinity_results_path}\")\n",
    "        # Parse confidence results\n",
    "        if os.path.exists(confidence_results_path):\n",
    "            with open(confidence_results_path, 'r') as f:\n",
    "                confidence_results = json.load(f)\n",
    "            # Extract the key values from confidence results\n",
    "            results.update({\n",
    "                \"confidence_score\": confidence_results.get(\"confidence_score\"),\n",
    "                \"ptm\": confidence_results.get(\"ptm\"),\n",
    "                \"iptm\": confidence_results.get(\"iptm\"),\n",
    "                \"ligand_iptm\": confidence_results.get(\"ligand_iptm\"),\n",
    "                \"protein_iptm\": confidence_results.get(\"protein_iptm\"),\n",
    "                \"complex_plddt\": confidence_results.get(\"complex_plddt\"),\n",
    "                \"complex_iplddt\": confidence_results.get(\"complex_iplddt\"),\n",
    "                \"complex_pde\": confidence_results.get(\"complex_pde\"),\n",
    "                \"complex_ipde\": confidence_results.get(\"complex_ipde\"),\n",
    "                \"chains_ptm\": confidence_results.get(\"chains_ptm\"),\n",
    "                \"pair_chains_iptm\": confidence_results.get(\"pair_chains_iptm\")\n",
    "            })\n",
    "        else:\n",
    "            # If confidence file doesn't exist, use default values\n",
    "            results.update({\n",
    "                \"confidence_score\": 0.0,\n",
    "                \"ptm\": 0.0,\n",
    "                \"iptm\": 0.0,\n",
    "                \"ligand_iptm\": 0.0,\n",
    "                \"protein_iptm\": 0.0,\n",
    "                \"complex_plddt\": 0.0,\n",
    "                \"complex_iplddt\": 0.0,\n",
    "                \"complex_pde\": 0.0,\n",
    "                \"complex_ipde\": 0.0,\n",
    "                \"chains_ptm\": {\"0\": 0.0, \"1\": 0.0},\n",
    "                \"pair_chains_iptm\": {\"0\": {\"0\": 0.0, \"1\": 0.0}, \"1\": {\"0\": 0.0, \"1\": 0.0}}\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing Boltz results: {str(e)}\")\n",
    "\n",
    "print(\"âœ… Utility functions loaded for structure prediction\")\n",
    "\n",
    "# Use the computational settings already configured above\n",
    "print(f\"Using GPU: {use_gpu}\")\n",
    "print(f\"Recycling steps: {recycling_steps}\")\n",
    "print(f\"Sampling steps: {sampling_steps}\")\n",
    "print(f\"Diffusion samples: {diffusion_samples}\")\n",
    "print(f\"Max parallel samples: {max_parallel_samples}\")\n",
    "print(f\"Step scale: {step_scale}\")\n",
    "print(f\"Affinity sampling steps: {sampling_steps_affinity}\")\n",
    "print(f\"Affinity diffusion samples: {diffusion_samples_affinity}\")\n",
    "print(f\"Affinity MW correction: {affinity_mw_correction}\")\n",
    "print(f\"Max MSA seqs: {max_msa_seqs}\")\n",
    "print(f\"Subsample MSA: {subsample_msa}\")\n",
    "print(f\"Num subsampled MSA: {num_subsampled_msa}\")\n",
    "\n",
    "# Run predictions for all designs\n",
    "prediction_results = []\n",
    "\n",
    "for yaml_path in yaml_files:\n",
    "    try:\n",
    "        # Extract design info from YAML filename\n",
    "        yaml_filename = os.path.basename(yaml_path)\n",
    "        design_name = os.path.splitext(yaml_filename)[0].replace(\"drug_screening_\", \"\")\n",
    "        \n",
    "        # Extract design information directly from YAML file\n",
    "        try:\n",
    "            with open(yaml_path, \"r\") as f:\n",
    "                yaml_content = yaml.safe_load(f)\n",
    "            \n",
    "            # Extract protein sequence and ligand SMILES from YAML\n",
    "            protein_sequence = \"\"\n",
    "            ligand_smiles = \"\"\n",
    "            \n",
    "            for seq_entry in yaml_content.get(\"sequences\", []):\n",
    "                if \"protein\" in seq_entry:\n",
    "                    protein_sequence = seq_entry[\"protein\"][\"sequence\"]\n",
    "                elif \"ligand\" in seq_entry:\n",
    "                    ligand_smiles = seq_entry[\"ligand\"][\"smiles\"]\n",
    "            \n",
    "            if not protein_sequence:\n",
    "                protein_sequence = \"Unknown\"\n",
    "            if not ligand_smiles:\n",
    "                ligand_smiles = \"Unknown\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract design info from YAML: {e}\")\n",
    "            protein_sequence = \"Unknown\"\n",
    "            ligand_smiles = \"Unknown\"\n",
    "\n",
    "        print(f\"ðŸ”¬ Running prediction for {design_name}...\")\n",
    "\n",
    "        # Run Boltz prediction with configured parameters\n",
    "        output = run_boltz_prediction(\n",
    "            yaml_path,\n",
    "            use_gpu=use_gpu,\n",
    "            override=True,\n",
    "            recycling_steps=recycling_steps,\n",
    "            sampling_steps=sampling_steps,\n",
    "            diffusion_samples=diffusion_samples,\n",
    "            max_parallel_samples=max_parallel_samples,\n",
    "            step_scale=step_scale,\n",
    "            affinity_mw_correction=affinity_mw_correction,\n",
    "            max_msa_seqs=max_msa_seqs,\n",
    "            sampling_steps_affinity=sampling_steps_affinity,\n",
    "            diffusion_samples_affinity=diffusion_samples_affinity,\n",
    "            subsample_msa=subsample_msa,\n",
    "            num_subsampled_msa=num_subsampled_msa\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Boltz prediction completed for {design_name}\")\n",
    "\n",
    "        # Parse results\n",
    "        results = parse_boltz_results(yaml_path)\n",
    "\n",
    "        # Store results\n",
    "        result_entry = {\n",
    "            \"Design Name\": design_name,\n",
    "            \"Protein Sequence\": protein_sequence,\n",
    "            \"Ligand SMILES\": ligand_smiles,\n",
    "            \"Affinity Pred Value\": results.get(\"affinity_pred_value\"),\n",
    "            \"Affinity Probability Binary\": results.get(\"affinity_probability_binary\"),\n",
    "            \"Confidence Score\": results.get(\"confidence_score\"),\n",
    "            \"pTM\": results.get(\"ptm\"),\n",
    "            \"ipTM\": results.get(\"iptm\"),\n",
    "            \"Ligand ipTM\": results.get(\"ligand_iptm\"),\n",
    "            \"Protein ipTM\": results.get(\"protein_iptm\"),\n",
    "            \"Complex pLDDT\": results.get(\"complex_plddt\"),\n",
    "            \"Complex ipLDDT\": results.get(\"complex_iplddt\"),\n",
    "            \"Complex PDE\": results.get(\"complex_pde\"),\n",
    "            \"Complex ipDE\": results.get(\"complex_ipde\")\n",
    "        }\n",
    "\n",
    "        # Calculate pIC50 if affinity value is available\n",
    "        if results.get(\"affinity_pred_value\"):\n",
    "            # Convert from nM to M and calculate pIC50\n",
    "            ic50_m = results[\"affinity_pred_value\"] * 1e-9\n",
    "            pic50 = -np.log10(ic50_m)\n",
    "            result_entry[\"pIC50\"] = pic50\n",
    "\n",
    "        prediction_results.append(result_entry)\n",
    "\n",
    "        print(f\"ðŸ“Š Results parsed for {design_name}\")\n",
    "        if results.get(\"affinity_pred_value\"):\n",
    "            print(f\"   Predicted IC50: {results['affinity_pred_value']:.2f} nM\")\n",
    "            if \"pIC50\" in result_entry:\n",
    "                print(f\"   Predicted pIC50: {result_entry['pIC50']:.2f}\")\n",
    "        if results.get(\"confidence_score\"):\n",
    "            print(f\"   Confidence Score: {results['confidence_score']:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {design_name}: {str(e)}\")\n",
    "        # Add entry with error information\n",
    "        prediction_results.append({\n",
    "            \"Design Name\": design_name,\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(prediction_results)\n",
    "\n",
    "# Display results summary\n",
    "print(f\"ðŸŽ¯ Prediction Summary:\")\n",
    "print(f\"Total designs processed: {len(yaml_files)}\")\n",
    "successful_predictions = len([r for r in prediction_results if \"Error\" not in r])\n",
    "print(f\"Successful predictions: {successful_predictions}\")\n",
    "print(f\"Failed predictions: {len(prediction_results) - successful_predictions}\")\n",
    "\n",
    "# Display results table\n",
    "if successful_predictions > 0:\n",
    "    print(\"ðŸ“‹ Prediction Results:\")\n",
    "    display_columns = [\"Design Name\", \"Affinity Pred Value\", \"pIC50\", \"Confidence Score\", \"Ligand ipTM\"]\n",
    "    available_columns = [col for col in display_columns if col in df_results.columns]\n",
    "    display(df_results[available_columns])\n",
    "else:\n",
    "    print(\"âš ï¸ No successful predictions to display.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@title 6ï¸âƒ£ ðŸ“Š Visualize Results & 3D Structures { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ### ðŸŽ¯ Plot Configuration\n",
    "\n",
    "#@markdown **Show all designs**: Display results for all designs or only successful predictions.\n",
    "show_all_designs = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Plot style**: Choose the visual style for your plots.\n",
    "plot_style = \"plotly_white\" #@param [\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\"]\n",
    "\n",
    "#@markdown **Color scheme**: Select color palette for visualizations.\n",
    "color_scheme = \"viridis\" #@param [\"viridis\", \"plasma\", \"inferno\", \"magma\", \"cividis\", \"rainbow\", \"turbo\"]\n",
    "\n",
    "#@markdown ### ðŸ§¬ 3D Structure Settings\n",
    "\n",
    "#@markdown **Structure style**: Choose how to display the 3D molecular structures.\n",
    "structure_style = \"cartoon\" #@param [\"cartoon\", \"stick\", \"sphere\", \"line\", \"surface\"]\n",
    "\n",
    "#@markdown **Show ligand**: Highlight the ligand in the 3D structure.\n",
    "show_ligand = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Ligand style**: Choose how to display the ligand.\n",
    "ligand_style = \"stick\" #@param [\"stick\", \"sphere\", \"line\", \"surface\"]\n",
    "\n",
    "#@markdown **Background color**: Set the background color for 3D viewer.\n",
    "background_color = \"white\" #@param [\"white\", \"black\", \"gray\", \"lightgray\"]\n",
    "\n",
    "# Import required libraries\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Install and import py3Dmol for 3D visualization\n",
    "try:\n",
    "    import py3Dmol\n",
    "except ImportError:\n",
    "    !pip install py3Dmol\n",
    "    import py3Dmol\n",
    "\n",
    "print(\"ðŸŽ¨ Setting up visualizations...\")\n",
    "\n",
    "# Filter data based on user preference\n",
    "if show_all_designs:\n",
    "    viz_data = df_results.copy()\n",
    "    print(f\"ðŸ“Š Visualizing all {len(viz_data)} designs\")\n",
    "else:\n",
    "    viz_data = df_results[~df_results.get(\"Error\", pd.Series(dtype=object)).notna()].copy()\n",
    "    print(f\"ðŸ“Š Visualizing {len(viz_data)} successful predictions\")\n",
    "\n",
    "if len(viz_data) == 0:\n",
    "    print(\"âš ï¸ No data available for visualization.\")\n",
    "else:\n",
    "    # Create visualization dashboard\n",
    "    print(\"ðŸŽ¯ Creating interactive plots...\")\n",
    "\n",
    "    # Set the plot template\n",
    "    import plotly.io as pio\n",
    "    pio.templates.default = plot_style\n",
    "\n",
    "    # 1. Affinity vs Confidence Scatter Plot\n",
    "    if \"Affinity Pred Value\" in viz_data.columns and \"Confidence Score\" in viz_data.columns:\n",
    "        fig1 = px.scatter(\n",
    "            viz_data,\n",
    "            x=\"Confidence Score\",\n",
    "            y=\"Affinity Pred Value\",\n",
    "            hover_data=[\"Design Name\"],\n",
    "            title=\"ðŸŽ¯ Predicted Affinity vs Confidence Score\",\n",
    "            labels={\n",
    "                \"Affinity Pred Value\": \"Predicted IC50 (nM)\",\n",
    "                \"Confidence Score\": \"Structure Confidence\"\n",
    "            },\n",
    "            color_discrete_sequence=px.colors.qualitative.Set1\n",
    "        )\n",
    "        fig1.update_layout(\n",
    "            height=500,\n",
    "            showlegend=False,\n",
    "            title_x=0.5,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        fig1.show()\n",
    "\n",
    "    # 2. pIC50 Distribution\n",
    "    if \"pIC50\" in viz_data.columns:\n",
    "        fig2 = px.histogram(\n",
    "            viz_data,\n",
    "            x=\"pIC50\",\n",
    "            nbins=20,\n",
    "            title=\"ðŸ“ˆ Distribution of Predicted pIC50 Values\",\n",
    "            labels={\"pIC50\": \"Predicted pIC50\", \"count\": \"Number of Designs\"},\n",
    "            color_discrete_sequence=[px.colors.qualitative.Set1[1]]\n",
    "        )\n",
    "        fig2.add_vline(\n",
    "            x=viz_data[\"pIC50\"].median(),\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            annotation_text=f\"Median: {viz_data[\"pIC50\"].median():.2f}\"\n",
    "        )\n",
    "        fig2.update_layout(\n",
    "            height=400,\n",
    "            showlegend=False,\n",
    "            title_x=0.5,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        fig2.show()\n",
    "\n",
    "    # 3. Multi-metric Comparison\n",
    "    metrics_cols = [\"Confidence Score\", \"Ligand ipTM\", \"Complex pLDDT\"]\n",
    "    available_metrics = [col for col in metrics_cols if col in viz_data.columns]\n",
    "\n",
    "    if len(available_metrics) > 1:\n",
    "        fig3 = make_subplots(\n",
    "            rows=1, cols=len(available_metrics),\n",
    "            subplot_titles=available_metrics,\n",
    "            shared_yaxis=True\n",
    "        )\n",
    "\n",
    "        colors = px.colors.qualitative.Set1[:len(available_metrics)]\n",
    "\n",
    "        for i, metric in enumerate(available_metrics):\n",
    "            fig3.add_trace(\n",
    "                go.Box(\n",
    "                    y=viz_data[metric],\n",
    "                    name=metric,\n",
    "                    marker_color=colors[i],\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "\n",
    "        fig3.update_layout(\n",
    "            title_text=\"ðŸ“Š Quality Metrics Distribution\",\n",
    "            height=400,\n",
    "            title_x=0.5,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        fig3.show()\n",
    "\n",
    "    # 4. Top Performers Table\n",
    "    if \"pIC50\" in viz_data.columns:\n",
    "        top_designs = viz_data.nlargest(5, \"pIC50\")\n",
    "        print(\"ðŸ† Top 5 Predicted Performers:\")\n",
    "        display_cols = [\"Design Name\", \"pIC50\", \"Affinity Pred Value\", \"Confidence Score\"]\n",
    "        available_display_cols = [col for col in display_cols if col in top_designs.columns]\n",
    "        display(top_designs[available_display_cols])\n",
    "\n",
    "    print(\"ðŸ§¬ Preparing 3D structure visualization...\")\n",
    "\n",
    "    # Find available PDB files\n",
    "    pdb_files = []\n",
    "    results_dir = \"catdiscovery_results\"\n",
    "\n",
    "    if os.path.exists(results_dir):\n",
    "        for design_name in viz_data[\"Design Name\"].unique():\n",
    "            # Look for PDB files in Boltz results\n",
    "            pattern = os.path.join(results_dir, f\"boltz_results_drug_screening_{design_name}\", \"predictions\", f\"drug_screening_{design_name}\", \"*.pdb\")\n",
    "            found_files = glob.glob(pattern)\n",
    "            if found_files:\n",
    "                pdb_files.extend([(design_name, f) for f in found_files])\n",
    "\n",
    "    if pdb_files:\n",
    "        print(f\"Found {len(pdb_files)} PDB structure files\")\n",
    "\n",
    "        # Create 3D visualization for top designs\n",
    "        max_structures = min(3, len(pdb_files))\n",
    "        print(f\"ðŸŽ­ Displaying top {max_structures} 3D structures...\")\n",
    "\n",
    "        for i, (design_name, pdb_file) in enumerate(pdb_files[:max_structures]):\n",
    "            print(f\"ðŸ”¬ Structure {i+1}: {design_name}\")\n",
    "\n",
    "            try:\n",
    "                # Read PDB file\n",
    "                with open(pdb_file, \"r\") as f:\n",
    "                    pdb_content = f.read()\n",
    "\n",
    "                # Create 3D viewer\n",
    "                viewer = py3Dmol.view(width=800, height=600)\n",
    "                viewer.addModel(pdb_content, \"pdb\")\n",
    "\n",
    "                # Set background\n",
    "                viewer.setBackgroundColor(background_color)\n",
    "\n",
    "                # Style protein structure\n",
    "                if structure_style == \"cartoon\":\n",
    "                    viewer.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "                elif structure_style == \"stick\":\n",
    "                    viewer.setStyle({\"stick\": {\"colorscheme\": \"chainHetatm\"}})\n",
    "                elif structure_style == \"sphere\":\n",
    "                    viewer.setStyle({\"sphere\": {\"colorscheme\": \"chainHetatm\"}})\n",
    "                elif structure_style == \"line\":\n",
    "                    viewer.setStyle({\"line\": {\"colorscheme\": \"chainHetatm\"}})\n",
    "                elif structure_style == \"surface\":\n",
    "                    viewer.addSurface(py3Dmol.VDW, {\"opacity\": 0.7, \"color\": \"white\"})\n",
    "\n",
    "                # Highlight ligand if requested\n",
    "                if show_ligand:\n",
    "                    if ligand_style == \"stick\":\n",
    "                        viewer.setStyle({\"hetflag\": True}, {\"stick\": {\"colorscheme\": \"greenCarbon\", \"radius\": 0.3}})\n",
    "                    elif ligand_style == \"sphere\":\n",
    "                        viewer.setStyle({\"hetflag\": True}, {\"sphere\": {\"colorscheme\": \"greenCarbon\", \"radius\": 0.8}})\n",
    "                    elif ligand_style == \"line\":\n",
    "                        viewer.setStyle({\"hetflag\": True}, {\"line\": {\"colorscheme\": \"greenCarbon\"}})\n",
    "                    elif ligand_style == \"surface\":\n",
    "                        viewer.addSurface(py3Dmol.VDW, {\"opacity\": 0.8, \"color\": \"green\"}, {\"hetflag\": True})\n",
    "\n",
    "                # Center and zoom\n",
    "                viewer.zoomTo()\n",
    "                viewer.show()\n",
    "\n",
    "                # Display structure info\n",
    "                if design_name in viz_data[\"Design Name\"].values:\n",
    "                    design_info = viz_data[viz_data[\"Design Name\"] == design_name].iloc[0]\n",
    "                    print(f\"ðŸ“Š Structure Quality Metrics:\")\n",
    "                    if \"Confidence Score\" in design_info and pd.notna(design_info[\"Confidence Score\"]):\n",
    "                        print(f\"   â€¢ Confidence Score: {design_info[\"Confidence Score\"]:.3f}\")\n",
    "                    if \"Ligand ipTM\" in design_info and pd.notna(design_info[\"Ligand ipTM\"]):\n",
    "                        print(f\"   â€¢ Ligand ipTM: {design_info[\"Ligand ipTM\"]:.3f}\")\n",
    "                    if \"pIC50\" in design_info and pd.notna(design_info[\"pIC50\"]):\n",
    "                        print(f\"   â€¢ Predicted pIC50: {design_info[\"pIC50\"]:.2f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error displaying structure for {design_name}: {str(e)}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No PDB structure files found. Make sure predictions have completed successfully.\")\n",
    "        print(f\"   Looking in: {results_dir}/boltz_results_*/predictions/*/\")\n",
    "\n",
    "print(\"âœ¨ Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips_section"
   },
   "source": [
    "## ðŸ’¡ Tips & Troubleshooting\n",
    "\n",
    "### ðŸ”§ Common Issues:\n",
    "\n",
    "1. Dependencies not installed: Re-run the setup cell and restart runtime\n",
    "2. Invalid SMILES: Check strings using online SMILES validators\n",
    "3. Invalid protein sequence: Use only standard amino acid codes (A-Z)\n",
    "4. GPU memory issues: Reduce batch size or switch to CPU mode\n",
    "\n",
    "### âš¡ Performance Tips:\n",
    "\n",
    "- Use GPU acceleration when available\n",
    "- Start with fewer combinations for testing\n",
    "- Monitor memory usage for large screens\n",
    "- Save intermediate results regularly\n",
    "\n",
    "---\n",
    "\n",
    "Happy Screening! ðŸ§¬ðŸ’Š"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
